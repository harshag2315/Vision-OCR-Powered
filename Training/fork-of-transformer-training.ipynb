{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11424699,"sourceType":"datasetVersion","datasetId":7125783},{"sourceId":11480663,"sourceType":"datasetVersion","datasetId":7195599}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport cv2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:35.652608Z","iopub.execute_input":"2025-04-28T18:31:35.653315Z","iopub.status.idle":"2025-04-28T18:31:35.656790Z","shell.execute_reply.started":"2025-04-28T18:31:35.653288Z","shell.execute_reply":"2025-04-28T18:31:35.656084Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:36.531087Z","iopub.execute_input":"2025-04-28T18:31:36.531389Z","iopub.status.idle":"2025-04-28T18:31:36.535316Z","shell.execute_reply.started":"2025-04-28T18:31:36.531367Z","shell.execute_reply":"2025-04-28T18:31:36.534568Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!git clone https://github.com/JaidedAI/EasyOCR.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:37.367211Z","iopub.execute_input":"2025-04-28T18:31:37.367502Z","iopub.status.idle":"2025-04-28T18:31:47.193738Z","shell.execute_reply.started":"2025-04-28T18:31:37.367482Z","shell.execute_reply":"2025-04-28T18:31:47.192954Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'EasyOCR'...\nremote: Enumerating objects: 2750, done.\u001b[K\nremote: Counting objects: 100% (661/661), done.\u001b[K\nremote: Compressing objects: 100% (86/86), done.\u001b[K\nremote: Total 2750 (delta 594), reused 575 (delta 575), pack-reused 2089 (from 1)\u001b[K\nReceiving objects: 100% (2750/2750), 157.82 MiB | 27.05 MiB/s, done.\nResolving deltas: 100% (1689/1689), done.\nUpdating files: 100% (313/313), done.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!cp -r /kaggle/working/EasyOCR/trainer /kaggle/working/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:53.512190Z","iopub.execute_input":"2025-04-28T18:31:53.512583Z","iopub.status.idle":"2025-04-28T18:31:53.648527Z","shell.execute_reply.started":"2025-04-28T18:31:53.512555Z","shell.execute_reply":"2025-04-28T18:31:53.647421Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"files = os.listdir(\"/kaggle/working/trainer\")\nfor file in files:\n    !cp -r /kaggle/working/trainer/{file} /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:53.720901Z","iopub.execute_input":"2025-04-28T18:31:53.721210Z","iopub.status.idle":"2025-04-28T18:31:55.197439Z","shell.execute_reply.started":"2025-04-28T18:31:53.721186Z","shell.execute_reply":"2025-04-28T18:31:55.196271Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!rm -r /kaggle/working/EasyOCR\n!rm -r /kaggle/working/trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:55.199146Z","iopub.execute_input":"2025-04-28T18:31:55.199434Z","iopub.status.idle":"2025-04-28T18:31:55.571361Z","shell.execute_reply.started":"2025-04-28T18:31:55.199413Z","shell.execute_reply":"2025-04-28T18:31:55.570341Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"os.mkdir(\"/kaggle/working/all_data/en_train_filtered\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:55.572757Z","iopub.execute_input":"2025-04-28T18:31:55.573064Z","iopub.status.idle":"2025-04-28T18:31:55.577330Z","shell.execute_reply.started":"2025-04-28T18:31:55.573035Z","shell.execute_reply":"2025-04-28T18:31:55.576423Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"%%time\n\n!cp -r /kaggle/input/trainer/all_data/en_train_filtered /kaggle/working/all_data/train\n!cp -r /kaggle/input/trainer/all_data/en_val /kaggle/working/all_data/validation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:55.579237Z","iopub.execute_input":"2025-04-28T18:31:55.579715Z","iopub.status.idle":"2025-04-28T18:31:59.793123Z","shell.execute_reply.started":"2025-04-28T18:31:55.579697Z","shell.execute_reply":"2025-04-28T18:31:59.792148Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 49.6 ms, sys: 29.9 ms, total: 79.5 ms\nWall time: 4.2 s\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"os.mkdir(\"/kaggle/working/all_data/en_train_filtered/__results___files\")\n\nimport os\nimport shutil\n\n# Define source and destination folders\nsrc_folder = \"/kaggle/working/all_data/train\"\ndst_folder = \"/kaggle/working/all_data/en_train_filtered/__results___files\"\n\n# Create destination folder if it doesn't exist\nos.makedirs(dst_folder, exist_ok=True)\n\n# Define allowed image extensions\nimage_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n\n# Move files\nmoved_count = 0\nfor file in os.listdir(src_folder):\n    if file.lower().endswith(image_extensions):\n        src_path = os.path.join(src_folder, file)\n        dst_path = os.path.join(dst_folder, file)\n        shutil.move(src_path, dst_path)\n        moved_count += 1\n\nprint(f\"✅ Moved {moved_count} image(s) from:\\n{src_folder}\\nto:\\n{dst_folder}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:59.794526Z","iopub.execute_input":"2025-04-28T18:31:59.795147Z","iopub.status.idle":"2025-04-28T18:31:59.832604Z","shell.execute_reply.started":"2025-04-28T18:31:59.795120Z","shell.execute_reply":"2025-04-28T18:31:59.831972Z"}},"outputs":[{"name":"stdout","text":"✅ Moved 899 image(s) from:\n/kaggle/working/all_data/train\nto:\n/kaggle/working/all_data/en_train_filtered/__results___files\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!cp /kaggle/input/dataset-old-label/labels.csv /kaggle/working/all_data/en_train_filtered/__results___files/labels.csv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:59.833409Z","iopub.execute_input":"2025-04-28T18:31:59.833699Z","iopub.status.idle":"2025-04-28T18:31:59.990194Z","shell.execute_reply.started":"2025-04-28T18:31:59.833677Z","shell.execute_reply":"2025-04-28T18:31:59.989313Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/all_data/en_val/__results___files\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:59.991395Z","iopub.execute_input":"2025-04-28T18:31:59.991749Z","iopub.status.idle":"2025-04-28T18:31:59.996417Z","shell.execute_reply.started":"2025-04-28T18:31:59.991714Z","shell.execute_reply":"2025-04-28T18:31:59.995788Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\nimport shutil\n\n# Define source and destination folders\nsrc_folder = \"/kaggle/working/all_data/validation\"\ndst_folder = \"/kaggle/working/all_data/en_val/__results___files\"\n\n# Create destination folder if it doesn't exist\nos.makedirs(dst_folder, exist_ok=True)\n\n# Define allowed image extensions\nimage_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n\n# Move files\nmoved_count = 0\nfor file in os.listdir(src_folder):\n    if file.lower().endswith(image_extensions):\n        src_path = os.path.join(src_folder, file)\n        dst_path = os.path.join(dst_folder, file)\n        shutil.move(src_path, dst_path)\n        moved_count += 1\n\nprint(f\"✅ Moved {moved_count} image(s) from:\\n{src_folder}\\nto:\\n{dst_folder}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:31:59.997371Z","iopub.execute_input":"2025-04-28T18:31:59.997630Z","iopub.status.idle":"2025-04-28T18:32:00.014366Z","shell.execute_reply.started":"2025-04-28T18:31:59.997605Z","shell.execute_reply":"2025-04-28T18:32:00.013744Z"}},"outputs":[{"name":"stdout","text":"✅ Moved 49 image(s) from:\n/kaggle/working/all_data/validation\nto:\n/kaggle/working/all_data/en_val/__results___files\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!cp /kaggle/input/dataset-old-label/val_labels.csv /kaggle/working/all_data/en_val/__results___files/labels.csv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:00.015132Z","iopub.execute_input":"2025-04-28T18:32:00.015401Z","iopub.status.idle":"2025-04-28T18:32:00.151357Z","shell.execute_reply.started":"2025-04-28T18:32:00.015381Z","shell.execute_reply":"2025-04-28T18:32:00.150337Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/working/all_data/en_train_filtered/__results___files/labels.csv\", sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\ntrain_df = train_df.dropna().reset_index(drop=True)\ntrain_df.to_csv(\"/kaggle/working/all_data/en_train_filtered/__results___files/labels.csv\",index=False)\n\nvalid_df = pd.read_csv(\"/kaggle/working/all_data/en_val/__results___files/labels.csv\", sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\nvalid_df = valid_df.dropna().reset_index(drop=True)\nvalid_df.to_csv(\"/kaggle/working/all_data/en_val/__results___files/labels.csv\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:00.153763Z","iopub.execute_input":"2025-04-28T18:32:00.154018Z","iopub.status.idle":"2025-04-28T18:32:00.312165Z","shell.execute_reply.started":"2025-04-28T18:32:00.153996Z","shell.execute_reply":"2025-04-28T18:32:00.311405Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/working/all_data/en_train_filtered/__results___files/labels.csv\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:00.313002Z","iopub.execute_input":"2025-04-28T18:32:00.313210Z","iopub.status.idle":"2025-04-28T18:32:00.322622Z","shell.execute_reply.started":"2025-04-28T18:32:00.313189Z","shell.execute_reply":"2025-04-28T18:32:00.321774Z"}},"outputs":[{"name":"stdout","text":"['filename', 'words']\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"os.listdir(\"all_data\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:00.323517Z","iopub.execute_input":"2025-04-28T18:32:00.323727Z","iopub.status.idle":"2025-04-28T18:32:00.333245Z","shell.execute_reply.started":"2025-04-28T18:32:00.323711Z","shell.execute_reply":"2025-04-28T18:32:00.332761Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"['en_train_filtered', 'folder.txt', 'en_val', 'validation', 'train']"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"import sys\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:00.334170Z","iopub.execute_input":"2025-04-28T18:32:00.334648Z","iopub.status.idle":"2025-04-28T18:32:00.345473Z","shell.execute_reply.started":"2025-04-28T18:32:00.334631Z","shell.execute_reply":"2025-04-28T18:32:00.344582Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"sys.path.append(\"/kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:00.346368Z","iopub.execute_input":"2025-04-28T18:32:00.347110Z","iopub.status.idle":"2025-04-28T18:32:00.356498Z","shell.execute_reply.started":"2025-04-28T18:32:00.347090Z","shell.execute_reply":"2025-04-28T18:32:00.355774Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"!pip install natsort\n\nimport os\nimport torch.backends.cudnn as cudnn\nimport yaml\nfrom utils import AttrDict\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:00.357472Z","iopub.execute_input":"2025-04-28T18:32:00.357815Z","iopub.status.idle":"2025-04-28T18:32:04.160467Z","shell.execute_reply.started":"2025-04-28T18:32:00.357794Z","shell.execute_reply":"2025-04-28T18:32:04.159418Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (8.4.0)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"cudnn.benchmark = True\ncudnn.deterministic = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:05.280937Z","iopub.execute_input":"2025-04-28T18:32:05.281921Z","iopub.status.idle":"2025-04-28T18:32:05.285951Z","shell.execute_reply.started":"2025-04-28T18:32:05.281883Z","shell.execute_reply":"2025-04-28T18:32:05.285108Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def get_config(file_path):\n    with open(file_path, 'r', encoding=\"utf8\") as stream:\n        opt = yaml.safe_load(stream)\n    opt = AttrDict(opt)\n    if opt.lang_char == 'None':\n        characters = ''\n        for data in opt['select_data'].split('-'):\n            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n            all_char = ''.join(df['words'])\n            characters += ''.join(set(all_char))\n        characters = sorted(set(characters))\n        opt.character= ''.join(characters)\n    else:\n        opt.character = opt.number + opt.symbol + opt.lang_char\n    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n    return opt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:05.479817Z","iopub.execute_input":"2025-04-28T18:32:05.480523Z","iopub.status.idle":"2025-04-28T18:32:05.485845Z","shell.execute_reply.started":"2025-04-28T18:32:05.480499Z","shell.execute_reply":"2025-04-28T18:32:05.485175Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"%%writefile config_files/en_filtered_config.yaml\nnumber: '0123456789'\nsymbol: \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €\"\nlang_char: 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\nexperiment_name: 'en_filtered'\ntrain_data: 'all_data'\nvalid_data: 'all_data/en_val'\nmanualSeed: 1111\nworkers: 4\nbatch_size: 16 # 32\nnum_iter: 10000\nvalInterval: 1000\nsaved_model: '' #'saved_models/en_filtered/iter_300000.pth'\nFT: False\noptim: False # default is Adadelta\nlr: 1.\nbeta1: 0.9\nrho: 0.95\neps: 0.00000001\ngrad_clip: 5\n#Data processing\nselect_data: 'en_train_filtered' # this is dataset folder in train_data\nbatch_ratio: '1' \ntotal_data_usage_ratio: 1.0\nbatch_max_length: 34 \nimgH: 64\nimgW: 600\nrgb: False\ncontrast_adjust: False\nsensitive: True\nPAD: True\ncontrast_adjust: 0.0\ndata_filtering_off: False\n# Model Architecture\nTransformation: 'None'\nFeatureExtraction: 'VGG'\nSequenceModeling: 'BiLSTM'\nPrediction: 'CTC'\nnum_fiducial: 20\ninput_channel: 1\noutput_channel: 256\nhidden_size: 256\ndecode: 'greedy'\nnew_prediction: False\nfreeze_FeatureFxtraction: False\nfreeze_SequenceModeling: False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:05.664336Z","iopub.execute_input":"2025-04-28T18:32:05.665059Z","iopub.status.idle":"2025-04-28T18:32:05.670941Z","shell.execute_reply.started":"2025-04-28T18:32:05.665033Z","shell.execute_reply":"2025-04-28T18:32:05.670311Z"}},"outputs":[{"name":"stdout","text":"Overwriting config_files/en_filtered_config.yaml\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"number: '0123456789'\nsymbol: \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €\"\nlang_char: 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\nexperiment_name: 'en_filtered'\ntrain_data: 'all_data'\nvalid_data: 'all_data/en_val'\nmanualSeed: 1111\nworkers: 4\nbatch_size: 16 # 32\nnum_iter: 10000\nvalInterval: 1000\nsaved_model: '' #'saved_models/en_filtered/iter_300000.pth'\nFT: False\noptim: False # default is Adadelta\nlr: 1.\nbeta1: 0.9\nrho: 0.95\neps: 0.00000001\ngrad_clip: 5\n#Data processing\nselect_data: 'en_train_filtered' # this is dataset folder in train_data\nbatch_ratio: '1' \ntotal_data_usage_ratio: 1.0\nbatch_max_length: 34 \nimgH: 64\nimgW: 600\nrgb: False\ncontrast_adjust: False\nsensitive: True\nPAD: True\ncontrast_adjust: 0.0\ndata_filtering_off: False\n# Model Architecture\nTransformation: 'None'\nFeatureExtraction: 'VGG'\nSequenceModeling: 'BiLSTM'\nPrediction: 'CTC'\nnum_fiducial: 20\ninput_channel: 1\noutput_channel: 256\nhidden_size: 256\ndecode: 'greedy'\nnew_prediction: False\nfreeze_FeatureFxtraction: False\nfreeze_SequenceModeling: False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:05.842812Z","iopub.execute_input":"2025-04-28T18:32:05.843444Z","iopub.status.idle":"2025-04-28T18:32:05.849719Z","shell.execute_reply.started":"2025-04-28T18:32:05.843420Z","shell.execute_reply":"2025-04-28T18:32:05.848852Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"opt = get_config(\"config_files/en_filtered_config.yaml\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:06.019399Z","iopub.execute_input":"2025-04-28T18:32:06.019713Z","iopub.status.idle":"2025-04-28T18:32:06.027605Z","shell.execute_reply.started":"2025-04-28T18:32:06.019687Z","shell.execute_reply":"2025-04-28T18:32:06.026894Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"%%writefile /kaggle/working/modules/sequence_modeling.py\n\nimport torch\nimport torch.nn as nn\n\n\nimport torch\nimport torch.nn as nn\n\nclass BidirectionalLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout=0.3):\n        super(BidirectionalLSTM, self).__init__()\n        \n        self.rnn = nn.LSTM(\n            input_size,\n            hidden_size,\n            bidirectional=True,\n            batch_first=True\n        )\n        \n        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n        self.dropout = nn.Dropout(dropout)\n        \n        self.fc = nn.Linear(hidden_size * 2, output_size)\n        \n        self._init_weights()\n\n    def _init_weights(self):\n        # Better initialization for small data\n        for name, param in self.rnn.named_parameters():\n            if 'weight' in name:\n                nn.init.xavier_uniform_(param)\n            elif 'bias' in name:\n                nn.init.constant_(param, 0)\n\n        nn.init.xavier_uniform_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n\n    def forward(self, x):\n        \"\"\"\n        x : visual feature [batch_size x T x input_size]\n        output : contextual feature [batch_size x T x output_size]\n        \"\"\"\n        try:\n            self.rnn.flatten_parameters()\n        except:\n            pass\n        \n        recurrent, _ = self.rnn(x)  # batch_size x T x (2*hidden_size)\n        \n        recurrent = self.layer_norm(recurrent)\n        recurrent = self.dropout(recurrent)\n        \n        output = self.fc(recurrent)  # batch_size x T x output_size\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:06.210017Z","iopub.execute_input":"2025-04-28T18:32:06.210767Z","iopub.status.idle":"2025-04-28T18:32:06.215773Z","shell.execute_reply.started":"2025-04-28T18:32:06.210741Z","shell.execute_reply":"2025-04-28T18:32:06.215130Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/modules/sequence_modeling.py\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"%%writefile /kaggle/working/dataset.py\n\nimport os\nimport sys\nimport re\nimport six\nimport math\nimport torch\nimport pandas  as pd\n\nfrom natsort import natsorted\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data import Dataset, ConcatDataset, Subset\nfrom itertools import accumulate\nimport torchvision.transforms as transforms\ndevice = torch.device('cuda')\ndef contrast_grey(img):\n    high = np.percentile(img, 90)\n    low  = np.percentile(img, 10)\n    return (high-low)/(high+low), high, low\n\ndef adjust_contrast_grey(img, target = 0.4):\n    contrast, high, low = contrast_grey(img)\n    if contrast < target:\n        img = img.astype(int)\n        ratio = 200./(high-low)\n        img = (img - low + 25)*ratio\n        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)\n    return img\n\n\nclass Batch_Balanced_Dataset(object):\n\n    def __init__(self, opt):\n        \"\"\"\n        Modulate the data ratio in the batch.\n        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n        \"\"\"\n        log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a')\n        dashed_line = '-' * 80\n        print(dashed_line)\n        log.write(dashed_line + '\\n')\n        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n        assert len(opt.select_data) == len(opt.batch_ratio)\n\n        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust = opt.contrast_adjust)\n        self.data_loader_list = []\n        self.dataloader_iter_list = []\n        batch_size_list = []\n        Total_batch_size = 0\n        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n            print(dashed_line)\n            log.write(dashed_line + '\\n')\n            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n            total_number_dataset = len(_dataset)\n            log.write(_dataset_log)\n\n            \"\"\"\n            The total number of data can be modified with opt.total_data_usage_ratio.\n            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n            See 4.2 section in our paper.\n            \"\"\"\n            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n            indices = range(total_number_dataset)\n            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n                           for offset, length in zip(accumulate(dataset_split), dataset_split)]\n            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n            print(selected_d_log)\n            log.write(selected_d_log + '\\n')\n            batch_size_list.append(str(_batch_size))\n            Total_batch_size += _batch_size\n\n            _data_loader = torch.utils.data.DataLoader(\n                _dataset, batch_size=_batch_size,\n                shuffle=True,\n                num_workers=int(opt.workers), #prefetch_factor=2,persistent_workers=True,\n                collate_fn=_AlignCollate, pin_memory=True)\n            self.data_loader_list.append(_data_loader)\n            self.dataloader_iter_list.append(iter(_data_loader))\n\n        Total_batch_size_log = f'{dashed_line}\\n'\n        batch_size_sum = '+'.join(batch_size_list)\n        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n        Total_batch_size_log += f'{dashed_line}'\n        opt.batch_size = Total_batch_size\n\n        print(Total_batch_size_log)\n        log.write(Total_batch_size_log + '\\n')\n        log.close()\n\n    def get_batch(self):\n        balanced_batch_images = []\n        balanced_batch_texts = []\n\n        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n            try:\n                image,text = next(iter(data_loader_iter))\n                balanced_batch_images.append(image)\n                balanced_batch_texts += text\n            except StopIteration:\n                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n                image, text = next(iter(self.dataloader_iter_list[i]))\n                balanced_batch_images.append(image)\n                balanced_batch_texts += text\n            except ValueError:\n                pass\n\n        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n\n        return balanced_batch_images, balanced_batch_texts\n\n\ndef hierarchical_dataset(root, opt, select_data='/'):\n    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n    dataset_list = []\n    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n    print(dataset_log)\n    dataset_log += '\\n'\n    for dirpath, dirnames, filenames in os.walk(root+'/'):\n        if not dirnames:\n            select_flag = False\n            for selected_d in select_data:\n                if selected_d in dirpath:\n                    select_flag = True\n                    break\n\n            if select_flag:\n                dataset = OCRDataset(dirpath, opt)\n                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n                print(sub_dataset_log)\n                dataset_log += f'{sub_dataset_log}\\n'\n                dataset_list.append(dataset)\n\n    concatenated_dataset = ConcatDataset(dataset_list)\n\n    return concatenated_dataset, dataset_log\n\nclass OCRDataset(Dataset):\n\n    def __init__(self, root, opt):\n\n        self.root = root\n        self.opt = opt\n        print(root)\n        self.df = pd.read_csv(os.path.join(root,'labels.csv'), sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n        self.nSamples = len(self.df)\n\n        if self.opt.data_filtering_off:\n            self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n        else:\n            self.filtered_index_list = []\n            for index in range(self.nSamples):\n                label = self.df.at[index,'words']\n                try:\n                    if len(label) > self.opt.batch_max_length:\n                        continue\n                except:\n                    print(label)\n                out_of_char = f'[^{self.opt.character}]'\n                if re.search(out_of_char, label.lower()):\n                    continue\n                self.filtered_index_list.append(index)\n            self.nSamples = len(self.filtered_index_list)\n\n    def __len__(self):\n        return self.nSamples\n\n    def __getitem__(self, index):\n        index = self.filtered_index_list[index]\n        img_fname = self.df.at[index,'filename']\n        img_fpath = os.path.join(self.root, img_fname)\n        label = self.df.at[index,'words']\n\n        if self.opt.rgb:\n            img = Image.open(img_fpath).convert('RGB')  # for color image\n        else:\n            img = Image.open(img_fpath).convert('L')\n\n        if not self.opt.sensitive:\n            label = label.lower()\n\n        # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n        out_of_char = f'[^{self.opt.character}]'\n        label = re.sub(out_of_char, '', label)\n\n        return (img, label)\n\nclass ResizeNormalize(object):\n\n    def __init__(self, size, interpolation=Image.BICUBIC):\n        self.size = size\n        self.interpolation = interpolation\n        self.toTensor = transforms.ToTensor()\n\n    def __call__(self, img):\n        img = img.resize(self.size, self.interpolation)\n        img = self.toTensor(img)\n        img.sub_(0.5).div_(0.5)\n        return img\n\n\nclass NormalizePAD(object):\n\n    def __init__(self, max_size, PAD_type='right'):\n        self.toTensor = transforms.ToTensor()\n        self.max_size = max_size\n        self.max_width_half = math.floor(max_size[2] / 2)\n        self.PAD_type = PAD_type\n\n    def __call__(self, img):\n        img = self.toTensor(img)\n        img.sub_(0.5).div_(0.5)\n        c, h, w = img.size()\n        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n        Pad_img[:, :, :w] = img  # right pad\n        if self.max_size[2] != w:  # add border Pad\n            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n\n        return Pad_img\n\n\nclass AlignCollate(object):\n\n    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, contrast_adjust = 0.):\n        self.imgH = imgH\n        self.imgW = imgW\n        self.keep_ratio_with_pad = keep_ratio_with_pad\n        self.contrast_adjust = contrast_adjust\n\n    def __call__(self, batch):\n        batch = filter(lambda x: x is not None, batch)\n        images, labels = zip(*batch)\n\n        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n            resized_max_w = self.imgW\n            input_channel = 3 if images[0].mode == 'RGB' else 1\n            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n\n            resized_images = []\n            for image in images:\n                w, h = image.size\n\n                #### augmentation here - change contrast\n                if self.contrast_adjust > 0:\n                    image = np.array(image.convert(\"L\"))\n                    image = adjust_contrast_grey(image, target = self.contrast_adjust)\n                    image = Image.fromarray(image, 'L')\n\n                ratio = w / float(h)\n                if math.ceil(self.imgH * ratio) > self.imgW:\n                    resized_w = self.imgW\n                else:\n                    resized_w = math.ceil(self.imgH * ratio)\n\n                resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n                resized_images.append(transform(resized_image))\n                # resized_image.save('./image_test/%d_test.jpg' % w)\n\n            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n\n        else:\n            transform = ResizeNormalize((self.imgW, self.imgH))\n            image_tensors = [transform(image) for image in images]\n            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n\n        return image_tensors, labels\n\n\ndef tensor2im(image_tensor, imtype=np.uint8):\n    with torch.no_grad():\n        image_tensor = image_tensor.to(device)\n        output = model(image_tensor)  # Use GPU\n    # ... other GPU ops\n\n    # Now it's safe to move to CPU for visualization or saving\n    image_numpy = image_tensor.detach().cpu().float().numpy()\n    if image_numpy.shape[0] == 1:\n        image_numpy = np.tile(image_numpy, (3, 1, 1))\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    return image_numpy.astype(imtype)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:06.511476Z","iopub.execute_input":"2025-04-28T18:32:06.512358Z","iopub.status.idle":"2025-04-28T18:32:06.520874Z","shell.execute_reply.started":"2025-04-28T18:32:06.512333Z","shell.execute_reply":"2025-04-28T18:32:06.520291Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/dataset.py\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import os\nimport sys\nimport re\nimport six\nimport math\nimport torch\nimport pandas  as pd\n\nfrom natsort import natsorted\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data import Dataset, ConcatDataset, Subset\nfrom itertools import accumulate\nimport torchvision.transforms as transforms\ndevice = torch.device('cuda')\ndef contrast_grey(img):\n    high = np.percentile(img, 90)\n    low  = np.percentile(img, 10)\n    return (high-low)/(high+low), high, low\n\ndef adjust_contrast_grey(img, target = 0.4):\n    contrast, high, low = contrast_grey(img)\n    if contrast < target:\n        img = img.astype(int)\n        ratio = 200./(high-low)\n        img = (img - low + 25)*ratio\n        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)\n    return img\n\n\nclass Batch_Balanced_Dataset(object):\n\n    def __init__(self, opt):\n        \"\"\"\n        Modulate the data ratio in the batch.\n        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n        \"\"\"\n        log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a')\n        dashed_line = '-' * 80\n        print(dashed_line)\n        log.write(dashed_line + '\\n')\n        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n        assert len(opt.select_data) == len(opt.batch_ratio)\n\n        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust = opt.contrast_adjust)\n        self.data_loader_list = []\n        self.dataloader_iter_list = []\n        batch_size_list = []\n        Total_batch_size = 0\n        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n            print(dashed_line)\n            log.write(dashed_line + '\\n')\n            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n            total_number_dataset = len(_dataset)\n            log.write(_dataset_log)\n\n            \"\"\"\n            The total number of data can be modified with opt.total_data_usage_ratio.\n            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n            See 4.2 section in our paper.\n            \"\"\"\n            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n            indices = range(total_number_dataset)\n            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n                           for offset, length in zip(accumulate(dataset_split), dataset_split)]\n            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n            print(selected_d_log)\n            log.write(selected_d_log + '\\n')\n            batch_size_list.append(str(_batch_size))\n            Total_batch_size += _batch_size\n\n            _data_loader = torch.utils.data.DataLoader(\n                _dataset, batch_size=_batch_size,\n                shuffle=True,\n                num_workers=int(opt.workers), #prefetch_factor=2,persistent_workers=True,\n                collate_fn=_AlignCollate, pin_memory=True)\n            self.data_loader_list.append(_data_loader)\n            self.dataloader_iter_list.append(iter(_data_loader))\n\n        Total_batch_size_log = f'{dashed_line}\\n'\n        batch_size_sum = '+'.join(batch_size_list)\n        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n        Total_batch_size_log += f'{dashed_line}'\n        opt.batch_size = Total_batch_size\n\n        print(Total_batch_size_log)\n        log.write(Total_batch_size_log + '\\n')\n        log.close()\n\n    def get_batch(self):\n        balanced_batch_images = []\n        balanced_batch_texts = []\n\n        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n            try:\n                image,text = next(iter(data_loader_iter))\n                balanced_batch_images.append(image)\n                balanced_batch_texts += text\n            except StopIteration:\n                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n                image, text = next(iter(self.dataloader_iter_list[i]))\n                balanced_batch_images.append(image)\n                balanced_batch_texts += text\n            except ValueError:\n                pass\n\n        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n\n        return balanced_batch_images, balanced_batch_texts\n\n\ndef hierarchical_dataset(root, opt, select_data='/'):\n    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n    dataset_list = []\n    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n    print(dataset_log)\n    dataset_log += '\\n'\n    for dirpath, dirnames, filenames in os.walk(root+'/'):\n        if not dirnames:\n            select_flag = False\n            for selected_d in select_data:\n                if selected_d in dirpath:\n                    select_flag = True\n                    break\n\n            if select_flag:\n                dataset = OCRDataset(dirpath, opt)\n                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n                print(sub_dataset_log)\n                dataset_log += f'{sub_dataset_log}\\n'\n                dataset_list.append(dataset)\n\n    concatenated_dataset = ConcatDataset(dataset_list)\n\n    return concatenated_dataset, dataset_log\n\nclass OCRDataset(Dataset):\n\n    def __init__(self, root, opt):\n\n        self.root = root\n        self.opt = opt\n        print(root)\n        self.df = pd.read_csv(os.path.join(root,'labels.csv'), sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n        self.nSamples = len(self.df)\n\n        if self.opt.data_filtering_off:\n            self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n        else:\n            self.filtered_index_list = []\n            for index in range(self.nSamples):\n                label = self.df.at[index,'words']\n                try:\n                    if len(label) > self.opt.batch_max_length:\n                        continue\n                except:\n                    print(label)\n                out_of_char = f'[^{self.opt.character}]'\n                if re.search(out_of_char, label.lower()):\n                    continue\n                self.filtered_index_list.append(index)\n            self.nSamples = len(self.filtered_index_list)\n\n    def __len__(self):\n        return self.nSamples\n\n    def __getitem__(self, index):\n        index = self.filtered_index_list[index]\n        img_fname = self.df.at[index,'filename']\n        img_fpath = os.path.join(self.root, img_fname)\n        label = self.df.at[index,'words']\n\n        if self.opt.rgb:\n            img = Image.open(img_fpath).convert('RGB')  # for color image\n        else:\n            img = Image.open(img_fpath).convert('L')\n\n        if not self.opt.sensitive:\n            label = label.lower()\n\n        # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n        out_of_char = f'[^{self.opt.character}]'\n        label = re.sub(out_of_char, '', label)\n\n        return (img, label)\n\nclass ResizeNormalize(object):\n\n    def __init__(self, size, interpolation=Image.BICUBIC):\n        self.size = size\n        self.interpolation = interpolation\n        self.toTensor = transforms.ToTensor()\n\n    def __call__(self, img):\n        img = img.resize(self.size, self.interpolation)\n        img = self.toTensor(img)\n        img.sub_(0.5).div_(0.5)\n        return img\n\n\nclass NormalizePAD(object):\n\n    def __init__(self, max_size, PAD_type='right'):\n        self.toTensor = transforms.ToTensor()\n        self.max_size = max_size\n        self.max_width_half = math.floor(max_size[2] / 2)\n        self.PAD_type = PAD_type\n\n    def __call__(self, img):\n        img = self.toTensor(img)\n        img.sub_(0.5).div_(0.5)\n        c, h, w = img.size()\n        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n        Pad_img[:, :, :w] = img  # right pad\n        if self.max_size[2] != w:  # add border Pad\n            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n\n        return Pad_img\n\n\nclass AlignCollate(object):\n\n    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, contrast_adjust = 0.):\n        self.imgH = imgH\n        self.imgW = imgW\n        self.keep_ratio_with_pad = keep_ratio_with_pad\n        self.contrast_adjust = contrast_adjust\n\n    def __call__(self, batch):\n        batch = filter(lambda x: x is not None, batch)\n        images, labels = zip(*batch)\n\n        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n            resized_max_w = self.imgW\n            input_channel = 3 if images[0].mode == 'RGB' else 1\n            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n\n            resized_images = []\n            for image in images:\n                w, h = image.size\n\n                #### augmentation here - change contrast\n                if self.contrast_adjust > 0:\n                    image = np.array(image.convert(\"L\"))\n                    image = adjust_contrast_grey(image, target = self.contrast_adjust)\n                    image = Image.fromarray(image, 'L')\n\n                ratio = w / float(h)\n                if math.ceil(self.imgH * ratio) > self.imgW:\n                    resized_w = self.imgW\n                else:\n                    resized_w = math.ceil(self.imgH * ratio)\n\n                resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n                resized_images.append(transform(resized_image))\n                # resized_image.save('./image_test/%d_test.jpg' % w)\n\n            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n\n        else:\n            transform = ResizeNormalize((self.imgW, self.imgH))\n            image_tensors = [transform(image) for image in images]\n            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n\n        return image_tensors, labels\n\n\ndef tensor2im(image_tensor, imtype=np.uint8):\n    with torch.no_grad():\n        image_tensor = image_tensor.to(device)\n        output = model(image_tensor)  # Use GPU\n    # ... other GPU ops\n\n    # Now it's safe to move to CPU for visualization or saving\n    image_numpy = image_tensor.detach().cpu().float().numpy()\n    if image_numpy.shape[0] == 1:\n        image_numpy = np.tile(image_numpy, (3, 1, 1))\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    return image_numpy.astype(imtype)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:06.561829Z","iopub.execute_input":"2025-04-28T18:32:06.562066Z","iopub.status.idle":"2025-04-28T18:32:09.599939Z","shell.execute_reply.started":"2025-04-28T18:32:06.562047Z","shell.execute_reply":"2025-04-28T18:32:09.599396Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"%%writefile /kaggle/working/test.py\n\n\nimport os\nimport time\nimport string\nimport argparse\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.utils.data\nimport torch.nn.functional as F\nimport numpy as np\nfrom nltk.metrics.distance import edit_distance\n\nfrom utils import CTCLabelConverter, AttnLabelConverter, Averager\nfrom dataset import hierarchical_dataset, AlignCollate\nfrom model import Model\n\n\ndef validation(model, criterion, evaluation_loader, converter, opt, device):\n    \"\"\" validation or evaluation \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    n_correct = 0\n    norm_ED = 0\n    length_of_data = 0\n    infer_time = 0\n    valid_loss_avg = Averager()\n\n    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n        batch_size = image_tensors.size(0)\n        length_of_data = length_of_data + batch_size\n        image = image_tensors.to(device)\n        # For max length prediction\n        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n\n        text_for_loss, length_for_loss = converter.encode(labels, batch_max_length=opt.batch_max_length)\n        text_for_loss = text_for_loss.to(device)\n        length_for_loss = length_for_loss.to(device)\n        \n        start_time = time.time()\n        if 'CTC' in opt.Prediction:\n            preds = model(image, text_for_pred)\n            forward_time = time.time() - start_time\n\n            # Calculate evaluation loss for CTC decoder.\n            preds_size = torch.IntTensor([preds.size(1)] * batch_size).to(device)\n            # permute 'preds' to use CTCloss format\n            log_probs = preds.log_softmax(2).permute(1, 0, 2)\n            cost = criterion(log_probs, text_for_loss, preds_size, length_for_loss)\n\n\n            if opt.decode == 'greedy':\n                # Select max probabilty (greedy decoding) then decode index to character\n                _, preds_index = preds.max(2)\n                preds_index = preds_index.view(-1)\n                preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n            elif opt.decode == 'beamsearch':\n                preds_str = converter.decode_beamsearch(preds, beamWidth=2)\n\n        else:\n            preds = model(image, text_for_pred, is_train=False)\n            forward_time = time.time() - start_time\n\n            preds = preds[:, :text_for_loss.shape[1] - 1, :]\n            target = text_for_loss[:, 1:]  # without [GO] Symbol\n            cost = criterion(preds.contiguous().view(-1, preds.shape[-1]), target.contiguous().view(-1))\n\n            # select max probabilty (greedy decoding) then decode index to character\n            _, preds_index = preds.max(2)\n            preds_str = converter.decode(preds_index, length_for_pred)\n            labels = converter.decode(text_for_loss[:, 1:], length_for_loss)\n\n        infer_time += forward_time\n        valid_loss_avg.add(cost)\n\n        # calculate accuracy & confidence score\n        preds_prob = F.softmax(preds, dim=2)\n        preds_max_prob, _ = preds_prob.max(dim=2)\n        confidence_score_list = []\n        \n        for gt, pred, pred_max_prob in zip(labels, preds_str, preds_max_prob):\n            if 'Attn' in opt.Prediction:\n                gt = gt[:gt.find('[s]')]\n                pred_EOS = pred.find('[s]')\n                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n                pred_max_prob = pred_max_prob[:pred_EOS]\n\n            if pred == gt:\n                n_correct += 1\n\n            '''\n            (old version) ICDAR2017 DOST Normalized Edit Distance https://rrc.cvc.uab.es/?ch=7&com=tasks\n            \"For each word we calculate the normalized edit distance to the length of the ground truth transcription.\" \n            if len(gt) == 0:\n                norm_ED += 1\n            else:\n                norm_ED += edit_distance(pred, gt) / len(gt)\n            '''\n            \n            # ICDAR2019 Normalized Edit Distance \n            if len(gt) == 0 or len(pred) ==0:\n                norm_ED += 0\n            elif len(gt) > len(pred):\n                norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n            else:\n                norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n\n            # calculate confidence score (= multiply of pred_max_prob)\n            try:\n                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n            except:\n                confidence_score = 0  # for empty pred case, when prune after \"end of sentence\" token ([s])\n            confidence_score_list.append(confidence_score)\n            # print(pred, gt, pred==gt, confidence_score)\n\n    accuracy = n_correct / float(length_of_data) * 100\n    norm_ED = norm_ED / float(length_of_data) # ICDAR2019 Normalized Edit Distance\n\n    return valid_loss_avg.val(), accuracy, norm_ED, preds_str, confidence_score_list, labels, infer_time, length_of_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:09.600926Z","iopub.execute_input":"2025-04-28T18:32:09.601332Z","iopub.status.idle":"2025-04-28T18:32:09.607822Z","shell.execute_reply.started":"2025-04-28T18:32:09.601311Z","shell.execute_reply":"2025-04-28T18:32:09.606979Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/test.py\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import os\nimport time\nimport string\nimport argparse\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.utils.data\nimport torch.nn.functional as F\nimport numpy as np\nfrom nltk.metrics.distance import edit_distance\n\nfrom utils import CTCLabelConverter, AttnLabelConverter, Averager\nfrom dataset import hierarchical_dataset, AlignCollate\nfrom model import Model\n\ndef validation(model, criterion, evaluation_loader, converter, opt, device):\n    \"\"\" validation or evaluation \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    n_correct = 0\n    norm_ED = 0\n    length_of_data = 0\n    infer_time = 0\n    valid_loss_avg = Averager()\n\n    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n        batch_size = image_tensors.size(0)\n        length_of_data = length_of_data + batch_size\n        image = image_tensors.to(device)\n        # For max length prediction\n        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n\n        text_for_loss, length_for_loss = converter.encode(labels, batch_max_length=opt.batch_max_length)\n        text_for_loss = text_for_loss.to(device)\n        length_for_loss = length_for_loss.to(device)\n        \n        start_time = time.time()\n        if 'CTC' in opt.Prediction:\n            preds = model(image, text_for_pred)\n            forward_time = time.time() - start_time\n\n            # Calculate evaluation loss for CTC decoder.\n            preds_size = torch.IntTensor([preds.size(1)] * batch_size).to(device)\n            # permute 'preds' to use CTCloss format\n            log_probs = preds.log_softmax(2).permute(1, 0, 2)\n            cost = criterion(log_probs, text_for_loss, preds_size, length_for_loss)\n\n            if opt.decode == 'greedy':\n                # Select max probabilty (greedy decoding) then decode index to character\n                _, preds_index = preds.max(2)\n                preds_index = preds_index.view(-1)\n                preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n            elif opt.decode == 'beamsearch':\n                preds_str = converter.decode_beamsearch(preds, beamWidth=2)\n\n        else:\n            preds = model(image, text_for_pred, is_train=False)\n            forward_time = time.time() - start_time\n\n            preds = preds[:, :text_for_loss.shape[1] - 1, :]\n            target = text_for_loss[:, 1:]  # without [GO] Symbol\n            cost = criterion(preds.contiguous().view(-1, preds.shape[-1]), target.contiguous().view(-1))\n\n            # select max probabilty (greedy decoding) then decode index to character\n            _, preds_index = preds.max(2)\n            preds_str = converter.decode(preds_index, length_for_pred)\n            labels = converter.decode(text_for_loss[:, 1:], length_for_loss)\n\n        infer_time += forward_time\n        valid_loss_avg.add(cost)\n\n        # calculate accuracy & confidence score\n        preds_prob = F.softmax(preds, dim=2)\n        preds_max_prob, _ = preds_prob.max(dim=2)\n        confidence_score_list = []\n        \n        for gt, pred, pred_max_prob in zip(labels, preds_str, preds_max_prob):\n            if 'Attn' in opt.Prediction:\n                gt = gt[:gt.find('[s]')]\n                pred_EOS = pred.find('[s]')\n                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n                pred_max_prob = pred_max_prob[:pred_EOS]\n\n            if pred == gt:\n                n_correct += 1\n\n            '''\n            (old version) ICDAR2017 DOST Normalized Edit Distance https://rrc.cvc.uab.es/?ch=7&com=tasks\n            \"For each word we calculate the normalized edit distance to the length of the ground truth transcription.\" \n            if len(gt) == 0:\n                norm_ED += 1\n            else:\n                norm_ED += edit_distance(pred, gt) / len(gt)\n            '''\n            \n            # ICDAR2019 Normalized Edit Distance \n            if len(gt) == 0 or len(pred) ==0:\n                norm_ED += 0\n            elif len(gt) > len(pred):\n                norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n            else:\n                norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n\n            # calculate confidence score (= multiply of pred_max_prob)\n            try:\n                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n            except:\n                confidence_score = 0  # for empty pred case, when prune after \"end of sentence\" token ([s])\n            confidence_score_list.append(confidence_score)\n            # print(pred, gt, pred==gt, confidence_score)\n\n    accuracy = n_correct / float(length_of_data) * 100\n    norm_ED = norm_ED / float(length_of_data) # ICDAR2019 Normalized Edit Distance\n\n    return valid_loss_avg.val(), accuracy, norm_ED, preds_str, confidence_score_list, labels, infer_time, length_of_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:09.608589Z","iopub.execute_input":"2025-04-28T18:32:09.608802Z","iopub.status.idle":"2025-04-28T18:32:10.938071Z","shell.execute_reply.started":"2025-04-28T18:32:09.608752Z","shell.execute_reply":"2025-04-28T18:32:10.937310Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"%%writefile /kaggle/working/train.py\n\nimport os\nimport sys\nimport time\nimport random\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom torch.cuda.amp import autocast, GradScaler\nimport numpy as np\nimport string\nimport argparse\n\nfrom nltk.metrics.distance import edit_distance\nfrom utils import CTCLabelConverter, AttnLabelConverter, Averager\nfrom dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\nfrom model import Model\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef count_parameters(model):\n    print(\"Modules, Parameters\")\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        param = parameter.numel()\n        #table.add_row([name, param])\n        total_params+=param\n        print(name, param)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params\n\ndef train(opt, show_number = 2, amp=False):\n    \"\"\" dataset preparation \"\"\"\n    if not opt.data_filtering_off:\n        print('Filtering the images containing characters which are not in opt.character')\n        print('Filtering the images whose label is longer than opt.batch_max_length')\n\n    opt.select_data = opt.select_data.split('-')\n    opt.batch_ratio = opt.batch_ratio.split('-')\n    train_dataset = Batch_Balanced_Dataset(opt)\n    \n    log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a', encoding=\"utf8\")\n    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust=opt.contrast_adjust)\n    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=min(32, opt.batch_size),\n        shuffle=True,  # 'True' to check training progress with validation function.\n        num_workers=int(opt.workers), prefetch_factor=512,\n        collate_fn=AlignCollate_valid, pin_memory=True)\n    log.write(valid_dataset_log)\n    print('-' * 80)\n    log.write('-' * 80 + '\\n')\n    log.close()\n    \n    \"\"\" model configuration \"\"\"\n    if 'CTC' in opt.Prediction:\n        converter = CTCLabelConverter(opt.character)\n    else:\n        converter = AttnLabelConverter(opt.character)\n    opt.num_class = len(converter.character)\n\n    if opt.rgb:\n        opt.input_channel = 3\n    model = Model(opt)\n    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n          opt.SequenceModeling, opt.Prediction)\n\n    if opt.saved_model != '':\n        pretrained_dict = torch.load(opt.saved_model)\n        if opt.new_prediction:\n            model.Prediction = nn.Linear(model.SequenceModeling_output, len(pretrained_dict['module.Prediction.weight']))  \n        \n        model = torch.nn.DataParallel(model).to(device) \n        print(f'loading pretrained model from {opt.saved_model}')\n        if opt.FT:\n            model.load_state_dict(pretrained_dict, strict=False)\n        else:\n            model.load_state_dict(pretrained_dict)\n        if opt.new_prediction:\n            model.module.Prediction = nn.Linear(model.module.SequenceModeling_output, opt.num_class)  \n            for name, param in model.module.Prediction.named_parameters():\n                if 'bias' in name:\n                    init.constant_(param, 0.0)\n                elif 'weight' in name:\n                    init.kaiming_normal_(param)\n            model = model.to(device) \n    else:\n        # weight initialization\n        for name, param in model.named_parameters():\n            if 'localization_fc2' in name:\n                print(f'Skip {name} as it is already initialized')\n                continue\n            try:\n                if 'bias' in name:\n                    init.constant_(param, 0.0)\n                elif 'weight' in name:\n                    init.kaiming_normal_(param)\n            except Exception as e:  # for batchnorm.\n                if 'weight' in name:\n                    param.data.fill_(1)\n                continue\n        model = torch.nn.DataParallel(model).to(device)\n    \n    model.train() \n    print(\"Model:\")\n    print(model)\n    count_parameters(model)\n    \n    \"\"\" setup loss \"\"\"\n    if 'CTC' in opt.Prediction:\n        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n    else:\n        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n    # loss averager\n    loss_avg = Averager()\n\n    # freeze some layers\n    try:\n        if opt.freeze_FeatureFxtraction:\n            for param in model.module.FeatureExtraction.parameters():\n                param.requires_grad = False\n        if opt.freeze_SequenceModeling:\n            for param in model.module.SequenceModeling.parameters():\n                param.requires_grad = False\n    except:\n        pass\n    \n    # filter that only require gradient decent\n    filtered_parameters = []\n    params_num = []\n    for p in filter(lambda p: p.requires_grad, model.parameters()):\n        filtered_parameters.append(p)\n        params_num.append(np.prod(p.size()))\n    print('Trainable params num : ', sum(params_num))\n    # [print(name, p.numel()) for name, p in filter(lambda p: p[1].requires_grad, model.named_parameters())]\n\n    # setup optimizer\n    if opt.optim=='adam':\n        #optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n        optimizer = optim.Adam(filtered_parameters)\n    else:\n        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n    print(\"Optimizer:\")\n    print(optimizer)\n\n    \"\"\" final options \"\"\"\n    # print(opt)\n    with open(f'./saved_models/{opt.experiment_name}/opt.txt', 'a', encoding=\"utf8\") as opt_file:\n        opt_log = '------------ Options -------------\\n'\n        args = vars(opt)\n        for k, v in args.items():\n            opt_log += f'{str(k)}: {str(v)}\\n'\n        opt_log += '---------------------------------------\\n'\n        print(opt_log)\n        opt_file.write(opt_log)\n\n    \"\"\" start training \"\"\"\n    start_iter = 0\n    if opt.saved_model != '':\n        try:\n            start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n            print(f'continue to train, start_iter: {start_iter}')\n        except:\n            pass\n\n    start_time = time.time()\n    best_accuracy = -1\n    best_norm_ED = -1\n    i = start_iter\n\n    scaler = GradScaler()\n    t1= time.time()\n        \n    while(True):\n        # train part\n        optimizer.zero_grad(set_to_none=True)\n        \n        if amp:\n            with autocast():\n                image_tensors, labels = train_dataset.get_batch()\n                image = image_tensors.to(device)\n                text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n                batch_size = image.size(0)\n\n                if 'CTC' in opt.Prediction:\n                    preds = model(image, text).log_softmax(2)\n                    preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n                    preds = preds.permute(1, 0, 2)\n                    torch.backends.cudnn.enabled = False\n                    cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n                    torch.backends.cudnn.enabled = True\n                else:\n                    preds = model(image, text[:, :-1])  # align with Attention.forward\n                    target = text[:, 1:]  # without [GO] Symbol\n                    cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n            scaler.scale(cost).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            image_tensors, labels = train_dataset.get_batch()\n            image = image_tensors.to(device)\n            text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n            batch_size = image.size(0)\n            if 'CTC' in opt.Prediction:\n                preds = model(image, text).log_softmax(2)\n                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n                preds = preds.permute(1, 0, 2)\n                torch.backends.cudnn.enabled = False\n                cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n                torch.backends.cudnn.enabled = True\n            else:\n                preds = model(image, text[:, :-1])  # align with Attention.forward\n                target = text[:, 1:]  # without [GO] Symbol\n                cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n            cost.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip) \n            optimizer.step()\n        loss_avg.add(cost)\n\n        # validation part\n        if (i % opt.valInterval == 0) and (i!=0):\n            print('training time: ', time.time()-t1)\n            t1=time.time()\n            elapsed_time = time.time() - start_time\n            # for log\n            with open(f'./saved_models/{opt.experiment_name}/log_train.txt', 'a', encoding=\"utf8\") as log:\n                model.eval()\n                with torch.no_grad():\n                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels,\\\n                    infer_time, length_of_data = validation(model, criterion, valid_loader, converter, opt, device)\n                model.train()\n\n                # training loss and validation loss\n                loss_log = f'[{i}/{opt.num_iter}] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n                loss_avg.reset()\n\n                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.4f}'\n\n                # keep best accuracy model (on valid dataset)\n                if current_accuracy > best_accuracy:\n                    best_accuracy = current_accuracy\n                    torch.save(model.state_dict(), f'/kaggle/working/best_accuracy.pth')\n                if current_norm_ED > best_norm_ED:\n                    best_norm_ED = current_norm_ED\n                    torch.save(model.state_dict(), f'/kaggle/working/best_norm_ED.pth')\n                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.4f}'\n\n                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n                print(loss_model_log)\n                log.write(loss_model_log + '\\n')\n\n                # show some predicted results\n                dashed_line = '-' * 80\n                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n                \n                #show_number = min(show_number, len(labels))\n                \n                start = random.randint(0,len(labels) - show_number )    \n                for gt, pred, confidence in zip(labels[start:start+show_number], preds[start:start+show_number], confidence_score[start:start+show_number]):\n                    if 'Attn' in opt.Prediction:\n                        gt = gt[:gt.find('[s]')]\n                        pred = pred[:pred.find('[s]')]\n\n                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n                predicted_result_log += f'{dashed_line}'\n                print(predicted_result_log)\n                log.write(predicted_result_log + '\\n')\n                print('validation time: ', time.time()-t1)\n                t1=time.time()\n        # save model per 1e+4 iter.\n        if (i + 1) % 1000 == 0:\n            torch.save(\n                model.state_dict(), f'./saved_models/{opt.experiment_name}/iter_{i+1}.pth')\n        if i == opt.num_iter:\n            print('end the training')\n            break\n        i += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:10.939358Z","iopub.execute_input":"2025-04-28T18:32:10.939763Z","iopub.status.idle":"2025-04-28T18:32:10.950873Z","shell.execute_reply.started":"2025-04-28T18:32:10.939734Z","shell.execute_reply":"2025-04-28T18:32:10.949925Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/train.py\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport random\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom torch.cuda.amp import autocast, GradScaler\nimport numpy as np\nimport string\nimport argparse\n\nfrom nltk.metrics.distance import edit_distance\nfrom utils import CTCLabelConverter, AttnLabelConverter, Averager\nfrom dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\nfrom model import Model\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef count_parameters(model):\n    print(\"Modules, Parameters\")\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        param = parameter.numel()\n        #table.add_row([name, param])\n        total_params+=param\n        print(name, param)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params\n\ndef train(opt, show_number = 2, amp=False):\n    \"\"\" dataset preparation \"\"\"\n    if not opt.data_filtering_off:\n        print('Filtering the images containing characters which are not in opt.character')\n        print('Filtering the images whose label is longer than opt.batch_max_length')\n\n    opt.select_data = opt.select_data.split('-')\n    opt.batch_ratio = opt.batch_ratio.split('-')\n    train_dataset = Batch_Balanced_Dataset(opt)\n    \n    log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a', encoding=\"utf8\")\n    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust=opt.contrast_adjust)\n    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=min(32, opt.batch_size),\n        shuffle=True,  # 'True' to check training progress with validation function.\n        num_workers=int(opt.workers), prefetch_factor=512,\n        collate_fn=AlignCollate_valid, pin_memory=True)\n    log.write(valid_dataset_log)\n    print('-' * 80)\n    log.write('-' * 80 + '\\n')\n    log.close()\n    \n    \"\"\" model configuration \"\"\"\n    if 'CTC' in opt.Prediction:\n        converter = CTCLabelConverter(opt.character)\n    else:\n        converter = AttnLabelConverter(opt.character)\n    opt.num_class = len(converter.character)\n\n    if opt.rgb:\n        opt.input_channel = 3\n    model = Model(opt)\n    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n          opt.SequenceModeling, opt.Prediction)\n\n    if opt.saved_model != '':\n        pretrained_dict = torch.load(opt.saved_model)\n        if opt.new_prediction:\n            model.Prediction = nn.Linear(model.SequenceModeling_output, len(pretrained_dict['module.Prediction.weight']))  \n        \n        model = torch.nn.DataParallel(model).to(device) \n        print(f'loading pretrained model from {opt.saved_model}')\n        if opt.FT:\n            model.load_state_dict(pretrained_dict, strict=False)\n        else:\n            model.load_state_dict(pretrained_dict)\n        if opt.new_prediction:\n            model.module.Prediction = nn.Linear(model.module.SequenceModeling_output, opt.num_class)  \n            for name, param in model.module.Prediction.named_parameters():\n                if 'bias' in name:\n                    init.constant_(param, 0.0)\n                elif 'weight' in name:\n                    init.kaiming_normal_(param)\n            model = model.to(device) \n    else:\n        # weight initialization\n        for name, param in model.named_parameters():\n            if 'localization_fc2' in name:\n                print(f'Skip {name} as it is already initialized')\n                continue\n            try:\n                if 'bias' in name:\n                    init.constant_(param, 0.0)\n                elif 'weight' in name:\n                    init.kaiming_normal_(param)\n            except Exception as e:  # for batchnorm.\n                if 'weight' in name:\n                    param.data.fill_(1)\n                continue\n        model = torch.nn.DataParallel(model).to(device)\n    \n    model.train() \n    print(\"Model:\")\n    print(model)\n    count_parameters(model)\n    \n    \"\"\" setup loss \"\"\"\n    if 'CTC' in opt.Prediction:\n        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n    else:\n        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n    # loss averager\n    loss_avg = Averager()\n\n    # freeze some layers\n    try:\n        if opt.freeze_FeatureFxtraction:\n            for param in model.module.FeatureExtraction.parameters():\n                param.requires_grad = False\n        if opt.freeze_SequenceModeling:\n            for param in model.module.SequenceModeling.parameters():\n                param.requires_grad = False\n    except:\n        pass\n    \n    # filter that only require gradient decent\n    filtered_parameters = []\n    params_num = []\n    for p in filter(lambda p: p.requires_grad, model.parameters()):\n        filtered_parameters.append(p)\n        params_num.append(np.prod(p.size()))\n    print('Trainable params num : ', sum(params_num))\n    # [print(name, p.numel()) for name, p in filter(lambda p: p[1].requires_grad, model.named_parameters())]\n\n    # setup optimizer\n    if opt.optim=='adam':\n        #optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n        optimizer = optim.Adam(filtered_parameters)\n    else:\n        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n    print(\"Optimizer:\")\n    print(optimizer)\n\n    \"\"\" final options \"\"\"\n    # print(opt)\n    with open(f'./saved_models/{opt.experiment_name}/opt.txt', 'a', encoding=\"utf8\") as opt_file:\n        opt_log = '------------ Options -------------\\n'\n        args = vars(opt)\n        for k, v in args.items():\n            opt_log += f'{str(k)}: {str(v)}\\n'\n        opt_log += '---------------------------------------\\n'\n        print(opt_log)\n        opt_file.write(opt_log)\n\n    \"\"\" start training \"\"\"\n    start_iter = 0\n    if opt.saved_model != '':\n        try:\n            start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n            print(f'continue to train, start_iter: {start_iter}')\n        except:\n            pass\n\n    start_time = time.time()\n    best_accuracy = -1\n    best_norm_ED = -1\n    i = start_iter\n\n    scaler = GradScaler()\n    t1= time.time()\n        \n    while(True):\n        # train part\n        optimizer.zero_grad(set_to_none=True)\n        \n        if amp:\n            with autocast():\n                image_tensors, labels = train_dataset.get_batch()\n                image = image_tensors.to(device)\n                text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n                batch_size = image.size(0)\n\n                if 'CTC' in opt.Prediction:\n                    preds = model(image, text).log_softmax(2)\n                    preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n                    preds = preds.permute(1, 0, 2)\n                    torch.backends.cudnn.enabled = False\n                    cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n                    torch.backends.cudnn.enabled = True\n                else:\n                    preds = model(image, text[:, :-1])  # align with Attention.forward\n                    target = text[:, 1:]  # without [GO] Symbol\n                    cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n            scaler.scale(cost).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            image_tensors, labels = train_dataset.get_batch()\n            image = image_tensors.to(device)\n            text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n            batch_size = image.size(0)\n            if 'CTC' in opt.Prediction:\n                preds = model(image, text).log_softmax(2)\n                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n                preds = preds.permute(1, 0, 2)\n                torch.backends.cudnn.enabled = False\n                cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n                torch.backends.cudnn.enabled = True\n            else:\n                preds = model(image, text[:, :-1])  # align with Attention.forward\n                target = text[:, 1:]  # without [GO] Symbol\n                cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n            cost.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip) \n            optimizer.step()\n        loss_avg.add(cost)\n\n        # validation part\n        if (i % opt.valInterval == 0) and (i!=0):\n            model = model.to(device)\n            print('training time: ', time.time()-t1)\n            t1=time.time()\n            elapsed_time = time.time() - start_time\n            # for log\n            with open(f'./saved_models/{opt.experiment_name}/log_train.txt', 'a', encoding=\"utf8\") as log:\n                model.eval()\n                with torch.no_grad():\n                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels,\\\n                    infer_time, length_of_data = validation(model, criterion, valid_loader, converter, opt, device)\n                model.train()\n\n                # training loss and validation loss\n                loss_log = f'[{i}/{opt.num_iter}] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n                loss_avg.reset()\n\n                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.4f}'\n\n                # keep best accuracy model (on valid dataset)\n                if current_accuracy > best_accuracy:\n                    best_accuracy = current_accuracy\n                    torch.save(model.state_dict(), f'/kaggle/working/best_accuracy.pth')\n                if current_norm_ED > best_norm_ED:\n                    best_norm_ED = current_norm_ED\n                    torch.save(model.state_dict(), f'/kaggle/working/best_norm_ED.pth')\n                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.4f}'\n\n                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n                print(loss_model_log)\n                log.write(loss_model_log + '\\n')\n\n                # show some predicted results\n                dashed_line = '-' * 80\n                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n                \n                #show_number = min(show_number, len(labels))\n                \n                start = random.randint(0,len(labels) - show_number )    \n                for gt, pred, confidence in zip(labels[start:start+show_number], preds[start:start+show_number], confidence_score[start:start+show_number]):\n                    if 'Attn' in opt.Prediction:\n                        gt = gt[:gt.find('[s]')]\n                        pred = pred[:pred.find('[s]')]\n\n                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n                predicted_result_log += f'{dashed_line}'\n                print(predicted_result_log)\n                log.write(predicted_result_log + '\\n')\n                print('validation time: ', time.time()-t1)\n                t1=time.time()\n        # save model per 1e+4 iter.\n        if (i + 1) % 10000 == 0:\n            torch.save(\n                model.state_dict(), f'./saved_models/{opt.experiment_name}/iter_{i+1}.pth')\n        if i == opt.num_iter:\n            print('end the training')\n            break\n        i += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:22.504839Z","iopub.execute_input":"2025-04-28T18:32:22.505552Z","iopub.status.idle":"2025-04-28T18:32:22.534712Z","shell.execute_reply.started":"2025-04-28T18:32:22.505528Z","shell.execute_reply":"2025-04-28T18:32:22.534092Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"train(opt, amp=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:27.786522Z","iopub.execute_input":"2025-04-28T18:32:27.787146Z","iopub.status.idle":"2025-04-28T18:41:38.549429Z","shell.execute_reply.started":"2025-04-28T18:32:27.787123Z","shell.execute_reply":"2025-04-28T18:41:38.548526Z"}},"outputs":[{"name":"stdout","text":"Filtering the images containing characters which are not in opt.character\nFiltering the images whose label is longer than opt.batch_max_length\n--------------------------------------------------------------------------------\ndataset_root: all_data\nopt.select_data: ['en_train_filtered']\nopt.batch_ratio: ['1']\n--------------------------------------------------------------------------------\ndataset_root:    all_data\t dataset: en_train_filtered\nall_data/en_train_filtered/__results___files\nsub-directory:\t/en_train_filtered/__results___files\t num samples: 876\nnum total samples of en_train_filtered: 876 x 1.0 (total_data_usage_ratio) = 876\nnum samples of en_train_filtered per batch: 16 x 1.0 (batch_ratio) = 16\n--------------------------------------------------------------------------------\nTotal_batch_size: 16 = 16\n--------------------------------------------------------------------------------\ndataset_root:    all_data/en_val\t dataset: /\nall_data/en_val/__results___files\nsub-directory:\t/__results___files\t num samples: 46\n--------------------------------------------------------------------------------\nNo Transformation module specified\nmodel input parameters 64 600 20 1 256 256 97 34 None VGG BiLSTM CTC\nModel:\nDataParallel(\n  (module): Model(\n    (FeatureExtraction): VGG_FeatureExtractor(\n      (ConvNet): Sequential(\n        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU(inplace=True)\n        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): ReLU(inplace=True)\n        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (7): ReLU(inplace=True)\n        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (9): ReLU(inplace=True)\n        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n        (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (13): ReLU(inplace=True)\n        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (16): ReLU(inplace=True)\n        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n        (18): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n        (19): ReLU(inplace=True)\n      )\n    )\n    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n    (SequenceModeling): Sequential(\n      (0): BidirectionalLSTM(\n        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.3, inplace=False)\n        (fc): Linear(in_features=512, out_features=256, bias=True)\n      )\n      (1): BidirectionalLSTM(\n        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.3, inplace=False)\n        (fc): Linear(in_features=512, out_features=256, bias=True)\n      )\n    )\n    (Prediction): Linear(in_features=256, out_features=97, bias=True)\n  )\n)\nModules, Parameters\nmodule.FeatureExtraction.ConvNet.0.weight 288\nmodule.FeatureExtraction.ConvNet.0.bias 32\nmodule.FeatureExtraction.ConvNet.3.weight 18432\nmodule.FeatureExtraction.ConvNet.3.bias 64\nmodule.FeatureExtraction.ConvNet.6.weight 73728\nmodule.FeatureExtraction.ConvNet.6.bias 128\nmodule.FeatureExtraction.ConvNet.8.weight 147456\nmodule.FeatureExtraction.ConvNet.8.bias 128\nmodule.FeatureExtraction.ConvNet.11.weight 294912\nmodule.FeatureExtraction.ConvNet.12.weight 256\nmodule.FeatureExtraction.ConvNet.12.bias 256\nmodule.FeatureExtraction.ConvNet.14.weight 589824\nmodule.FeatureExtraction.ConvNet.15.weight 256\nmodule.FeatureExtraction.ConvNet.15.bias 256\nmodule.FeatureExtraction.ConvNet.18.weight 262144\nmodule.FeatureExtraction.ConvNet.18.bias 256\nmodule.SequenceModeling.0.rnn.weight_ih_l0 262144\nmodule.SequenceModeling.0.rnn.weight_hh_l0 262144\nmodule.SequenceModeling.0.rnn.bias_ih_l0 1024\nmodule.SequenceModeling.0.rnn.bias_hh_l0 1024\nmodule.SequenceModeling.0.rnn.weight_ih_l0_reverse 262144\nmodule.SequenceModeling.0.rnn.weight_hh_l0_reverse 262144\nmodule.SequenceModeling.0.rnn.bias_ih_l0_reverse 1024\nmodule.SequenceModeling.0.rnn.bias_hh_l0_reverse 1024\nmodule.SequenceModeling.0.layer_norm.weight 512\nmodule.SequenceModeling.0.layer_norm.bias 512\nmodule.SequenceModeling.0.fc.weight 131072\nmodule.SequenceModeling.0.fc.bias 256\nmodule.SequenceModeling.1.rnn.weight_ih_l0 262144\nmodule.SequenceModeling.1.rnn.weight_hh_l0 262144\nmodule.SequenceModeling.1.rnn.bias_ih_l0 1024\nmodule.SequenceModeling.1.rnn.bias_hh_l0 1024\nmodule.SequenceModeling.1.rnn.weight_ih_l0_reverse 262144\nmodule.SequenceModeling.1.rnn.weight_hh_l0_reverse 262144\nmodule.SequenceModeling.1.rnn.bias_ih_l0_reverse 1024\nmodule.SequenceModeling.1.rnn.bias_hh_l0_reverse 1024\nmodule.SequenceModeling.1.layer_norm.weight 512\nmodule.SequenceModeling.1.layer_norm.bias 512\nmodule.SequenceModeling.1.fc.weight 131072\nmodule.SequenceModeling.1.fc.bias 256\nmodule.Prediction.weight 24832\nmodule.Prediction.bias 97\nTotal Trainable Params: 3783393\nTrainable params num :  3783393\nOptimizer:\nAdadelta (\nParameter Group 0\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 1.0\n    maximize: False\n    rho: 0.95\n    weight_decay: 0\n)\n------------ Options -------------\nnumber: 0123456789\nsymbol: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €\nlang_char: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nexperiment_name: en_filtered\ntrain_data: all_data\nvalid_data: all_data/en_val\nmanualSeed: 1111\nworkers: 4\nbatch_size: 16\nnum_iter: 10000\nvalInterval: 1000\nsaved_model: \nFT: False\noptim: False\nlr: 1.0\nbeta1: 0.9\nrho: 0.95\neps: 1e-08\ngrad_clip: 5\nselect_data: ['en_train_filtered']\nbatch_ratio: ['1']\ntotal_data_usage_ratio: 1.0\nbatch_max_length: 34\nimgH: 64\nimgW: 600\nrgb: False\ncontrast_adjust: 0.0\nsensitive: True\nPAD: True\ndata_filtering_off: False\nTransformation: None\nFeatureExtraction: VGG\nSequenceModeling: BiLSTM\nPrediction: CTC\nnum_fiducial: 20\ninput_channel: 1\noutput_channel: 256\nhidden_size: 256\ndecode: greedy\nnew_prediction: False\nfreeze_FeatureFxtraction: False\nfreeze_SequenceModeling: False\ncharacter: 0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nnum_class: 97\n---------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/4257424241.py:176: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"training time:  58.80640435218811\n[1000/10000] Train loss: 5.09508, Valid loss: 3.42091, Elapsed_time: 58.80707\nCurrent_accuracy : 0.000, Current_norm_ED  : 0.0657\nBest_accuracy    : 0.000, Best_norm_ED     : 0.0657\n--------------------------------------------------------------------------------\nGround Truth              | Prediction                | Confidence Score & T/F\n--------------------------------------------------------------------------------\nKOHGILUYEH leasing        |                           | 0.0000\tFalse\nFukushima                 |                           | 0.0000\tFalse\n--------------------------------------------------------------------------------\nvalidation time:  0.9072506427764893\ntraining time:  54.054274559020996\n[2000/10000] Train loss: 2.31875, Valid loss: 0.97937, Elapsed_time: 113.76887\nCurrent_accuracy : 8.696, Current_norm_ED  : 0.7265\nBest_accuracy    : 8.696, Best_norm_ED     : 0.7265\n--------------------------------------------------------------------------------\nGround Truth              | Prediction                | Confidence Score & T/F\n--------------------------------------------------------------------------------\nComparative               | Comparatie                | 0.0089\tFalse\nInnocent Westlake Aeschinus | nnocent estake Aeschinus  | 0.0002\tFalse\n--------------------------------------------------------------------------------\nvalidation time:  0.5514552593231201\ntraining time:  53.98236083984375\n[3000/10000] Train loss: 0.45492, Valid loss: 0.14637, Elapsed_time: 168.30296\nCurrent_accuracy : 60.870, Current_norm_ED  : 0.9524\nBest_accuracy    : 60.870, Best_norm_ED     : 0.9524\n--------------------------------------------------------------------------------\nGround Truth              | Prediction                | Confidence Score & T/F\n--------------------------------------------------------------------------------\nMortality Qualifications  | Mortality Qualifications  | 0.0035\tTrue\nBALTIC :                  | BALTILC :                 | 0.0949\tFalse\n--------------------------------------------------------------------------------\nvalidation time:  0.5779571533203125\ntraining time:  53.94090700149536\n[4000/10000] Train loss: 0.10934, Valid loss: 0.01854, Elapsed_time: 222.82210\nCurrent_accuracy : 93.478, Current_norm_ED  : 0.9949\nBest_accuracy    : 93.478, Best_norm_ED     : 0.9949\n--------------------------------------------------------------------------------\nGround Truth              | Prediction                | Confidence Score & T/F\n--------------------------------------------------------------------------------\n631 Kamhlaba              | 631 Kamhlaba              | 0.4613\tTrue\nSwamy Carys Personified   | Swamy Carys Personified   | 0.0306\tTrue\n--------------------------------------------------------------------------------\nvalidation time:  0.5604777336120605\ntraining time:  53.9720299243927\n[5000/10000] Train loss: 0.04862, Valid loss: 0.00935, Elapsed_time: 277.35489\nCurrent_accuracy : 95.652, Current_norm_ED  : 0.9962\nBest_accuracy    : 95.652, Best_norm_ED     : 0.9962\n--------------------------------------------------------------------------------\nGround Truth              | Prediction                | Confidence Score & T/F\n--------------------------------------------------------------------------------\nSwamy Carys Personified   | Swamy Carys Personified   | 0.0717\tTrue\nUmpires                   | Umpires                   | 0.7440\tTrue\n--------------------------------------------------------------------------------\nvalidation time:  0.5564641952514648\ntraining time:  53.987358808517456\n[6000/10000] Train loss: 0.03179, Valid loss: 0.00455, Elapsed_time: 331.89899\nCurrent_accuracy : 97.826, Current_norm_ED  : 0.9989\nBest_accuracy    : 97.826, Best_norm_ED     : 0.9989\n--------------------------------------------------------------------------------\nGround Truth              | Prediction                | Confidence Score & T/F\n--------------------------------------------------------------------------------\nBALTIC :                  | BALTIC :                  | 0.6972\tTrue\nInnocent Westlake Aeschinus | Innocent Westlake Aeschinus | 0.1968\tTrue\n--------------------------------------------------------------------------------\nvalidation time:  0.5620229244232178\ntraining time:  53.769989252090454\n[7000/10000] Train loss: 0.02449, Valid loss: 0.00200, Elapsed_time: 386.23127\nCurrent_accuracy : 97.826, Current_norm_ED  : 0.9992\nBest_accuracy    : 97.826, Best_norm_ED     : 0.9992\n--------------------------------------------------------------------------------\nGround Truth              | Prediction                | Confidence Score & T/F\n--------------------------------------------------------------------------------\nBALTIC :                  | BALTIC :                  | 0.3182\tTrue\ncustomizable ] 908172     | customizable ] 908172     | 0.1822\tTrue\n--------------------------------------------------------------------------------\nvalidation time:  0.5273599624633789\ntraining time:  53.97143578529358\n[8000/10000] Train loss: 0.01752, Valid loss: 0.00178, Elapsed_time: 440.73035\nCurrent_accuracy : 100.000, Current_norm_ED  : 1.0000\nBest_accuracy    : 100.000, Best_norm_ED     : 1.0000\n--------------------------------------------------------------------------------\nGround Truth              | Prediction                | Confidence Score & T/F\n--------------------------------------------------------------------------------\n547190 soddyite Partick   | 547190 soddyite Partick   | 0.4142\tTrue\nHeavenly Truman immorality | Heavenly Truman immorality | 0.4860\tTrue\n--------------------------------------------------------------------------------\nvalidation time:  0.5785403251647949\ntraining time:  53.90199613571167\n[9000/10000] Train loss: 0.01487, Valid loss: 0.00041, Elapsed_time: 495.21119\nCurrent_accuracy : 100.000, Current_norm_ED  : 1.0000\nBest_accuracy    : 100.000, Best_norm_ED     : 1.0000\n--------------------------------------------------------------------------------\nGround Truth              | Prediction                | Confidence Score & T/F\n--------------------------------------------------------------------------------\nEnough Wiener             | Enough Wiener             | 0.2537\tTrue\nSpecimens Militia % Jiang | Specimens Militia % Jiang | 0.5977\tTrue\n--------------------------------------------------------------------------------\nvalidation time:  0.47332167625427246\ntraining time:  53.975515365600586\n[10000/10000] Train loss: 0.01140, Valid loss: 0.00005, Elapsed_time: 549.66032\nCurrent_accuracy : 100.000, Current_norm_ED  : 1.0000\nBest_accuracy    : 100.000, Best_norm_ED     : 1.0000\n--------------------------------------------------------------------------------\nGround Truth              | Prediction                | Confidence Score & T/F\n--------------------------------------------------------------------------------\nKETTERING                 | KETTERING                 | 0.1821\tTrue\nFukushima                 | Fukushima                 | 0.4944\tTrue\n--------------------------------------------------------------------------------\nvalidation time:  0.4825417995452881\nend the training\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}