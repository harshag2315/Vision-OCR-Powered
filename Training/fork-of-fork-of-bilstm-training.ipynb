{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c430e300",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-30T08:06:40.357880Z",
     "iopub.status.busy": "2025-04-30T08:06:40.357566Z",
     "iopub.status.idle": "2025-04-30T08:06:42.252541Z",
     "shell.execute_reply": "2025-04-30T08:06:42.251764Z"
    },
    "papermill": {
     "duration": 1.905013,
     "end_time": "2025-04-30T08:06:42.254448",
     "exception": false,
     "start_time": "2025-04-30T08:06:40.349435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e59c0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:06:42.274157Z",
     "iopub.status.busy": "2025-04-30T08:06:42.273471Z",
     "iopub.status.idle": "2025-04-30T08:06:46.290091Z",
     "shell.execute_reply": "2025-04-30T08:06:46.289466Z"
    },
    "papermill": {
     "duration": 4.026376,
     "end_time": "2025-04-30T08:06:46.291391",
     "exception": false,
     "start_time": "2025-04-30T08:06:42.265015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a10988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:06:46.305226Z",
     "iopub.status.busy": "2025-04-30T08:06:46.304610Z",
     "iopub.status.idle": "2025-04-30T08:06:55.232366Z",
     "shell.execute_reply": "2025-04-30T08:06:55.231594Z"
    },
    "papermill": {
     "duration": 8.935354,
     "end_time": "2025-04-30T08:06:55.233759",
     "exception": false,
     "start_time": "2025-04-30T08:06:46.298405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EasyOCR'...\r\n",
      "remote: Enumerating objects: 2750, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (869/869), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (106/106), done.\u001b[K\r\n",
      "remote: Total 2750 (delta 790), reused 763 (delta 763), pack-reused 1881 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (2750/2750), 157.82 MiB | 30.02 MiB/s, done.\r\n",
      "Resolving deltas: 100% (1690/1690), done.\r\n",
      "Updating files: 100% (313/313), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/JaidedAI/EasyOCR.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a076cfc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:06:55.250057Z",
     "iopub.status.busy": "2025-04-30T08:06:55.249320Z",
     "iopub.status.idle": "2025-04-30T08:06:55.378315Z",
     "shell.execute_reply": "2025-04-30T08:06:55.377419Z"
    },
    "papermill": {
     "duration": 0.138638,
     "end_time": "2025-04-30T08:06:55.379834",
     "exception": false,
     "start_time": "2025-04-30T08:06:55.241196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/working/EasyOCR/trainer /kaggle/working/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42108907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:06:55.395498Z",
     "iopub.status.busy": "2025-04-30T08:06:55.395192Z",
     "iopub.status.idle": "2025-04-30T08:06:56.866089Z",
     "shell.execute_reply": "2025-04-30T08:06:56.865016Z"
    },
    "papermill": {
     "duration": 1.480485,
     "end_time": "2025-04-30T08:06:56.867526",
     "exception": false,
     "start_time": "2025-04-30T08:06:55.387041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = os.listdir(\"/kaggle/working/trainer\")\n",
    "for file in files:\n",
    "    !cp -r /kaggle/working/trainer/{file} /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e281f446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:06:56.882981Z",
     "iopub.status.busy": "2025-04-30T08:06:56.882706Z",
     "iopub.status.idle": "2025-04-30T08:06:57.253389Z",
     "shell.execute_reply": "2025-04-30T08:06:57.252485Z"
    },
    "papermill": {
     "duration": 0.379927,
     "end_time": "2025-04-30T08:06:57.254882",
     "exception": false,
     "start_time": "2025-04-30T08:06:56.874955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -r /kaggle/working/EasyOCR\n",
    "!rm -r /kaggle/working/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d4c95ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:06:57.270604Z",
     "iopub.status.busy": "2025-04-30T08:06:57.269958Z",
     "iopub.status.idle": "2025-04-30T08:06:57.274205Z",
     "shell.execute_reply": "2025-04-30T08:06:57.273450Z"
    },
    "papermill": {
     "duration": 0.013376,
     "end_time": "2025-04-30T08:06:57.275546",
     "exception": false,
     "start_time": "2025-04-30T08:06:57.262170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"/kaggle/working/all_data/en_train_filtered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60fb366b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:06:57.290333Z",
     "iopub.status.busy": "2025-04-30T08:06:57.290072Z",
     "iopub.status.idle": "2025-04-30T08:07:06.919974Z",
     "shell.execute_reply": "2025-04-30T08:07:06.919000Z"
    },
    "papermill": {
     "duration": 9.638571,
     "end_time": "2025-04-30T08:07:06.921235",
     "exception": false,
     "start_time": "2025-04-30T08:06:57.282664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 126 ms, sys: 35.3 ms, total: 162 ms\n",
      "Wall time: 9.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!cp -r /kaggle/input/trainer/all_data/en_train_filtered /kaggle/working/all_data/train\n",
    "!cp -r /kaggle/input/trainer/all_data/en_val /kaggle/working/all_data/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79e6559c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:06.936590Z",
     "iopub.status.busy": "2025-04-30T08:07:06.936298Z",
     "iopub.status.idle": "2025-04-30T08:07:06.968260Z",
     "shell.execute_reply": "2025-04-30T08:07:06.967354Z"
    },
    "papermill": {
     "duration": 0.041071,
     "end_time": "2025-04-30T08:07:06.969502",
     "exception": false,
     "start_time": "2025-04-30T08:07:06.928431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Moved 899 image(s) from:\n",
      "/kaggle/working/all_data/train\n",
      "to:\n",
      "/kaggle/working/all_data/en_train_filtered/__results___files\n"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"/kaggle/working/all_data/en_train_filtered/__results___files\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define source and destination folders\n",
    "src_folder = \"/kaggle/working/all_data/train\"\n",
    "dst_folder = \"/kaggle/working/all_data/en_train_filtered/__results___files\"\n",
    "\n",
    "# Create destination folder if it doesn't exist\n",
    "os.makedirs(dst_folder, exist_ok=True)\n",
    "\n",
    "# Define allowed image extensions\n",
    "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "\n",
    "# Move files\n",
    "moved_count = 0\n",
    "for file in os.listdir(src_folder):\n",
    "    if file.lower().endswith(image_extensions):\n",
    "        src_path = os.path.join(src_folder, file)\n",
    "        dst_path = os.path.join(dst_folder, file)\n",
    "        shutil.move(src_path, dst_path)\n",
    "        moved_count += 1\n",
    "\n",
    "print(f\"✅ Moved {moved_count} image(s) from:\\n{src_folder}\\nto:\\n{dst_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131fe67b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:06.985468Z",
     "iopub.status.busy": "2025-04-30T08:07:06.985217Z",
     "iopub.status.idle": "2025-04-30T08:07:07.122369Z",
     "shell.execute_reply": "2025-04-30T08:07:07.121460Z"
    },
    "papermill": {
     "duration": 0.146387,
     "end_time": "2025-04-30T08:07:07.123846",
     "exception": false,
     "start_time": "2025-04-30T08:07:06.977459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/dataset-old-label/labels.csv /kaggle/working/all_data/en_train_filtered/__results___files/labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d18ad6c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:07.139316Z",
     "iopub.status.busy": "2025-04-30T08:07:07.139034Z",
     "iopub.status.idle": "2025-04-30T08:07:07.143095Z",
     "shell.execute_reply": "2025-04-30T08:07:07.142514Z"
    },
    "papermill": {
     "duration": 0.013157,
     "end_time": "2025-04-30T08:07:07.144165",
     "exception": false,
     "start_time": "2025-04-30T08:07:07.131008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"/kaggle/working/all_data/en_val/__results___files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b3b7a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:07.158937Z",
     "iopub.status.busy": "2025-04-30T08:07:07.158683Z",
     "iopub.status.idle": "2025-04-30T08:07:07.165748Z",
     "shell.execute_reply": "2025-04-30T08:07:07.165016Z"
    },
    "papermill": {
     "duration": 0.015733,
     "end_time": "2025-04-30T08:07:07.166910",
     "exception": false,
     "start_time": "2025-04-30T08:07:07.151177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Moved 49 image(s) from:\n",
      "/kaggle/working/all_data/validation\n",
      "to:\n",
      "/kaggle/working/all_data/en_val/__results___files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define source and destination folders\n",
    "src_folder = \"/kaggle/working/all_data/validation\"\n",
    "dst_folder = \"/kaggle/working/all_data/en_val/__results___files\"\n",
    "\n",
    "# Create destination folder if it doesn't exist\n",
    "os.makedirs(dst_folder, exist_ok=True)\n",
    "\n",
    "# Define allowed image extensions\n",
    "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "\n",
    "# Move files\n",
    "moved_count = 0\n",
    "for file in os.listdir(src_folder):\n",
    "    if file.lower().endswith(image_extensions):\n",
    "        src_path = os.path.join(src_folder, file)\n",
    "        dst_path = os.path.join(dst_folder, file)\n",
    "        shutil.move(src_path, dst_path)\n",
    "        moved_count += 1\n",
    "\n",
    "print(f\"✅ Moved {moved_count} image(s) from:\\n{src_folder}\\nto:\\n{dst_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "287a9e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:07.181791Z",
     "iopub.status.busy": "2025-04-30T08:07:07.181529Z",
     "iopub.status.idle": "2025-04-30T08:07:07.320272Z",
     "shell.execute_reply": "2025-04-30T08:07:07.319357Z"
    },
    "papermill": {
     "duration": 0.147747,
     "end_time": "2025-04-30T08:07:07.321871",
     "exception": false,
     "start_time": "2025-04-30T08:07:07.174124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/dataset-old-label/val_labels.csv /kaggle/working/all_data/en_val/__results___files/labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c70a2bf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:07.337220Z",
     "iopub.status.busy": "2025-04-30T08:07:07.336924Z",
     "iopub.status.idle": "2025-04-30T08:07:07.369510Z",
     "shell.execute_reply": "2025-04-30T08:07:07.368894Z"
    },
    "papermill": {
     "duration": 0.041841,
     "end_time": "2025-04-30T08:07:07.370931",
     "exception": false,
     "start_time": "2025-04-30T08:07:07.329090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/working/all_data/en_train_filtered/__results___files/labels.csv\", sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "train_df = train_df.dropna().reset_index(drop=True)\n",
    "train_df.to_csv(\"/kaggle/working/all_data/en_train_filtered/__results___files/labels.csv\",index=False)\n",
    "\n",
    "valid_df = pd.read_csv(\"/kaggle/working/all_data/en_val/__results___files/labels.csv\", sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "valid_df = valid_df.dropna().reset_index(drop=True)\n",
    "valid_df.to_csv(\"/kaggle/working/all_data/en_val/__results___files/labels.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca9e18fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:07.386636Z",
     "iopub.status.busy": "2025-04-30T08:07:07.386097Z",
     "iopub.status.idle": "2025-04-30T08:07:07.395360Z",
     "shell.execute_reply": "2025-04-30T08:07:07.394716Z"
    },
    "papermill": {
     "duration": 0.017968,
     "end_time": "2025-04-30T08:07:07.396427",
     "exception": false,
     "start_time": "2025-04-30T08:07:07.378459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['filename', 'words']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/working/all_data/en_train_filtered/__results___files/labels.csv\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89029277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:07.411245Z",
     "iopub.status.busy": "2025-04-30T08:07:07.411022Z",
     "iopub.status.idle": "2025-04-30T08:07:07.417172Z",
     "shell.execute_reply": "2025-04-30T08:07:07.416363Z"
    },
    "papermill": {
     "duration": 0.014762,
     "end_time": "2025-04-30T08:07:07.418338",
     "exception": false,
     "start_time": "2025-04-30T08:07:07.403576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'validation', 'en_val', 'en_train_filtered', 'folder.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"all_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba94bb91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:07.433641Z",
     "iopub.status.busy": "2025-04-30T08:07:07.433020Z",
     "iopub.status.idle": "2025-04-30T08:07:07.436366Z",
     "shell.execute_reply": "2025-04-30T08:07:07.435814Z"
    },
    "papermill": {
     "duration": 0.011831,
     "end_time": "2025-04-30T08:07:07.437337",
     "exception": false,
     "start_time": "2025-04-30T08:07:07.425506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9e1aebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:07.452144Z",
     "iopub.status.busy": "2025-04-30T08:07:07.451681Z",
     "iopub.status.idle": "2025-04-30T08:07:07.455073Z",
     "shell.execute_reply": "2025-04-30T08:07:07.454510Z"
    },
    "papermill": {
     "duration": 0.011811,
     "end_time": "2025-04-30T08:07:07.456102",
     "exception": false,
     "start_time": "2025-04-30T08:07:07.444291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/kaggle/working/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "648b541f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:07.471694Z",
     "iopub.status.busy": "2025-04-30T08:07:07.471034Z",
     "iopub.status.idle": "2025-04-30T08:07:11.337759Z",
     "shell.execute_reply": "2025-04-30T08:07:11.337002Z"
    },
    "papermill": {
     "duration": 3.876405,
     "end_time": "2025-04-30T08:07:11.339738",
     "exception": false,
     "start_time": "2025-04-30T08:07:07.463333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (8.4.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install natsort\n",
    "\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import yaml\n",
    "from utils import AttrDict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43ffef5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:11.356416Z",
     "iopub.status.busy": "2025-04-30T08:07:11.355792Z",
     "iopub.status.idle": "2025-04-30T08:07:11.359510Z",
     "shell.execute_reply": "2025-04-30T08:07:11.358967Z"
    },
    "papermill": {
     "duration": 0.012709,
     "end_time": "2025-04-30T08:07:11.360467",
     "exception": false,
     "start_time": "2025-04-30T08:07:11.347758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb1fea40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:11.375597Z",
     "iopub.status.busy": "2025-04-30T08:07:11.375386Z",
     "iopub.status.idle": "2025-04-30T08:07:11.380778Z",
     "shell.execute_reply": "2025-04-30T08:07:11.380057Z"
    },
    "papermill": {
     "duration": 0.014318,
     "end_time": "2025-04-30T08:07:11.381854",
     "exception": false,
     "start_time": "2025-04-30T08:07:11.367536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_config(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    opt = AttrDict(opt)\n",
    "    if opt.lang_char == 'None':\n",
    "        characters = ''\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n",
    "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character= ''.join(characters)\n",
    "    else:\n",
    "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
    "    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0c46798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:11.396553Z",
     "iopub.status.busy": "2025-04-30T08:07:11.396327Z",
     "iopub.status.idle": "2025-04-30T08:07:11.401892Z",
     "shell.execute_reply": "2025-04-30T08:07:11.401341Z"
    },
    "papermill": {
     "duration": 0.014031,
     "end_time": "2025-04-30T08:07:11.402902",
     "exception": false,
     "start_time": "2025-04-30T08:07:11.388871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config_files/en_filtered_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config_files/en_filtered_config.yaml\n",
    "number: '0123456789'\n",
    "symbol: \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €\"\n",
    "lang_char: 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "experiment_name: 'en_filtered'\n",
    "train_data: 'all_data'\n",
    "valid_data: 'all_data/en_val'\n",
    "manualSeed: 1111\n",
    "workers: 4\n",
    "batch_size: 16 # 32\n",
    "num_iter: 10000\n",
    "valInterval: 1000\n",
    "saved_model: '' #'saved_models/en_filtered/iter_300000.pth'\n",
    "FT: False\n",
    "optim: False # default is Adadelta\n",
    "lr: 1.\n",
    "beta1: 0.9\n",
    "rho: 0.95\n",
    "eps: 0.00000001\n",
    "grad_clip: 5\n",
    "#Data processing\n",
    "select_data: 'en_train_filtered' # this is dataset folder in train_data\n",
    "batch_ratio: '1' \n",
    "total_data_usage_ratio: 1.0\n",
    "batch_max_length: 34 \n",
    "imgH: 64\n",
    "imgW: 600\n",
    "rgb: False\n",
    "contrast_adjust: False\n",
    "sensitive: True\n",
    "PAD: True\n",
    "contrast_adjust: 0.0\n",
    "data_filtering_off: False\n",
    "# Model Architecture\n",
    "Transformation: 'None'\n",
    "FeatureExtraction: 'VGG'\n",
    "SequenceModeling: 'BiLSTM'\n",
    "Prediction: 'CTC'\n",
    "num_fiducial: 20\n",
    "input_channel: 1\n",
    "output_channel: 256\n",
    "hidden_size: 256\n",
    "decode: 'greedy'\n",
    "new_prediction: False\n",
    "freeze_FeatureFxtraction: False\n",
    "freeze_SequenceModeling: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a823b71c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:11.418875Z",
     "iopub.status.busy": "2025-04-30T08:07:11.418183Z",
     "iopub.status.idle": "2025-04-30T08:07:11.424101Z",
     "shell.execute_reply": "2025-04-30T08:07:11.423631Z"
    },
    "papermill": {
     "duration": 0.014942,
     "end_time": "2025-04-30T08:07:11.425227",
     "exception": false,
     "start_time": "2025-04-30T08:07:11.410285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "number: '0123456789'\n",
    "symbol: \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €\"\n",
    "lang_char: 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "experiment_name: 'en_filtered'\n",
    "train_data: 'all_data'\n",
    "valid_data: 'all_data/en_val'\n",
    "manualSeed: 1111\n",
    "workers: 4\n",
    "batch_size: 16 # 32\n",
    "num_iter: 10000\n",
    "valInterval: 1000\n",
    "saved_model: '' #'saved_models/en_filtered/iter_300000.pth'\n",
    "FT: False\n",
    "optim: False # default is Adadelta\n",
    "lr: 1.\n",
    "beta1: 0.9\n",
    "rho: 0.95\n",
    "eps: 0.00000001\n",
    "grad_clip: 5\n",
    "#Data processing\n",
    "select_data: 'en_train_filtered' # this is dataset folder in train_data\n",
    "batch_ratio: '1' \n",
    "total_data_usage_ratio: 1.0\n",
    "batch_max_length: 34 \n",
    "imgH: 64\n",
    "imgW: 600\n",
    "rgb: False\n",
    "contrast_adjust: False\n",
    "sensitive: True\n",
    "PAD: True\n",
    "contrast_adjust: 0.0\n",
    "data_filtering_off: False\n",
    "# Model Architecture\n",
    "Transformation: 'None'\n",
    "FeatureExtraction: 'VGG'\n",
    "SequenceModeling: 'BiLSTM'\n",
    "Prediction: 'CTC'\n",
    "num_fiducial: 20\n",
    "input_channel: 1\n",
    "output_channel: 256\n",
    "hidden_size: 256\n",
    "decode: 'greedy'\n",
    "new_prediction: False\n",
    "freeze_FeatureFxtraction: False\n",
    "freeze_SequenceModeling: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dc44e21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:11.440538Z",
     "iopub.status.busy": "2025-04-30T08:07:11.439909Z",
     "iopub.status.idle": "2025-04-30T08:07:11.446725Z",
     "shell.execute_reply": "2025-04-30T08:07:11.446192Z"
    },
    "papermill": {
     "duration": 0.01523,
     "end_time": "2025-04-30T08:07:11.447724",
     "exception": false,
     "start_time": "2025-04-30T08:07:11.432494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = get_config(\"config_files/en_filtered_config.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b5b809d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:11.516862Z",
     "iopub.status.busy": "2025-04-30T08:07:11.516331Z",
     "iopub.status.idle": "2025-04-30T08:07:11.521447Z",
     "shell.execute_reply": "2025-04-30T08:07:11.520638Z"
    },
    "papermill": {
     "duration": 0.014373,
     "end_time": "2025-04-30T08:07:11.522644",
     "exception": false,
     "start_time": "2025-04-30T08:07:11.508271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/modules/sequence_modeling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/modules/sequence_modeling.py\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size, hidden_size,\n",
    "            bidirectional=True, batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): visual feature [batch_size, T, input_size]\n",
    "        Returns:\n",
    "            Tensor: contextual feature [batch_size, T, output_size]\n",
    "        \"\"\"\n",
    "        if self.training:  # flatten_parameters only when training to save memory\n",
    "            self.rnn.flatten_parameters()\n",
    "\n",
    "        rnn_out, _ = self.rnn(x)  # [batch_size, T, 2*hidden_size]\n",
    "        out = self.linear(rnn_out)  # [batch_size, T, output_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b969fc3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:11.538728Z",
     "iopub.status.busy": "2025-04-30T08:07:11.538331Z",
     "iopub.status.idle": "2025-04-30T08:07:11.546555Z",
     "shell.execute_reply": "2025-04-30T08:07:11.545868Z"
    },
    "papermill": {
     "duration": 0.017347,
     "end_time": "2025-04-30T08:07:11.547572",
     "exception": false,
     "start_time": "2025-04-30T08:07:11.530225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/dataset.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import six\n",
    "import math\n",
    "import torch\n",
    "import pandas  as pd\n",
    "\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
    "from itertools import accumulate\n",
    "import torchvision.transforms as transforms\n",
    "device = torch.device('cuda')\n",
    "def contrast_grey(img):\n",
    "    high = np.percentile(img, 90)\n",
    "    low  = np.percentile(img, 10)\n",
    "    return (high-low)/(high+low), high, low\n",
    "\n",
    "def adjust_contrast_grey(img, target = 0.4):\n",
    "    contrast, high, low = contrast_grey(img)\n",
    "    if contrast < target:\n",
    "        img = img.astype(int)\n",
    "        ratio = 200./(high-low)\n",
    "        img = (img - low + 25)*ratio\n",
    "        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Batch_Balanced_Dataset(object):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"\n",
    "        Modulate the data ratio in the batch.\n",
    "        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n",
    "        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n",
    "        \"\"\"\n",
    "        log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a')\n",
    "        dashed_line = '-' * 80\n",
    "        print(dashed_line)\n",
    "        log.write(dashed_line + '\\n')\n",
    "        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n",
    "        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n",
    "        assert len(opt.select_data) == len(opt.batch_ratio)\n",
    "\n",
    "        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust = opt.contrast_adjust)\n",
    "        self.data_loader_list = []\n",
    "        self.dataloader_iter_list = []\n",
    "        batch_size_list = []\n",
    "        Total_batch_size = 0\n",
    "        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n",
    "            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n",
    "            print(dashed_line)\n",
    "            log.write(dashed_line + '\\n')\n",
    "            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n",
    "            total_number_dataset = len(_dataset)\n",
    "            log.write(_dataset_log)\n",
    "\n",
    "            \"\"\"\n",
    "            The total number of data can be modified with opt.total_data_usage_ratio.\n",
    "            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n",
    "            See 4.2 section in our paper.\n",
    "            \"\"\"\n",
    "            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n",
    "            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n",
    "            indices = range(total_number_dataset)\n",
    "            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n",
    "                           for offset, length in zip(accumulate(dataset_split), dataset_split)]\n",
    "            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n",
    "            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n",
    "            print(selected_d_log)\n",
    "            log.write(selected_d_log + '\\n')\n",
    "            batch_size_list.append(str(_batch_size))\n",
    "            Total_batch_size += _batch_size\n",
    "\n",
    "            _data_loader = torch.utils.data.DataLoader(\n",
    "                _dataset, batch_size=_batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=int(opt.workers), #prefetch_factor=2,persistent_workers=True,\n",
    "                collate_fn=_AlignCollate, pin_memory=True)\n",
    "            self.data_loader_list.append(_data_loader)\n",
    "            self.dataloader_iter_list.append(iter(_data_loader))\n",
    "\n",
    "        Total_batch_size_log = f'{dashed_line}\\n'\n",
    "        batch_size_sum = '+'.join(batch_size_list)\n",
    "        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n",
    "        Total_batch_size_log += f'{dashed_line}'\n",
    "        opt.batch_size = Total_batch_size\n",
    "\n",
    "        print(Total_batch_size_log)\n",
    "        log.write(Total_batch_size_log + '\\n')\n",
    "        log.close()\n",
    "\n",
    "    def get_batch(self):\n",
    "        balanced_batch_images = []\n",
    "        balanced_batch_texts = []\n",
    "\n",
    "        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n",
    "            try:\n",
    "                image,text = next(iter(data_loader_iter))\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except StopIteration:\n",
    "                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n",
    "                image, text = next(iter(self.dataloader_iter_list[i]))\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n",
    "\n",
    "        return balanced_batch_images, balanced_batch_texts\n",
    "\n",
    "\n",
    "def hierarchical_dataset(root, opt, select_data='/'):\n",
    "    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n",
    "    dataset_list = []\n",
    "    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n",
    "    print(dataset_log)\n",
    "    dataset_log += '\\n'\n",
    "    for dirpath, dirnames, filenames in os.walk(root+'/'):\n",
    "        if not dirnames:\n",
    "            select_flag = False\n",
    "            for selected_d in select_data:\n",
    "                if selected_d in dirpath:\n",
    "                    select_flag = True\n",
    "                    break\n",
    "\n",
    "            if select_flag:\n",
    "                dataset = OCRDataset(dirpath, opt)\n",
    "                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n",
    "                print(sub_dataset_log)\n",
    "                dataset_log += f'{sub_dataset_log}\\n'\n",
    "                dataset_list.append(dataset)\n",
    "\n",
    "    concatenated_dataset = ConcatDataset(dataset_list)\n",
    "\n",
    "    return concatenated_dataset, dataset_log\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, opt):\n",
    "\n",
    "        self.root = root\n",
    "        self.opt = opt\n",
    "        print(root)\n",
    "        self.df = pd.read_csv(os.path.join(root,'labels.csv'), sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "        self.nSamples = len(self.df)\n",
    "\n",
    "        if self.opt.data_filtering_off:\n",
    "            self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n",
    "        else:\n",
    "            self.filtered_index_list = []\n",
    "            for index in range(self.nSamples):\n",
    "                label = self.df.at[index,'words']\n",
    "                try:\n",
    "                    if len(label) > self.opt.batch_max_length:\n",
    "                        continue\n",
    "                except:\n",
    "                    print(label)\n",
    "                out_of_char = f'[^{self.opt.character}]'\n",
    "                if re.search(out_of_char, label.lower()):\n",
    "                    continue\n",
    "                self.filtered_index_list.append(index)\n",
    "            self.nSamples = len(self.filtered_index_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.filtered_index_list[index]\n",
    "        img_fname = self.df.at[index,'filename']\n",
    "        img_fpath = os.path.join(self.root, img_fname)\n",
    "        label = self.df.at[index,'words']\n",
    "\n",
    "        if self.opt.rgb:\n",
    "            img = Image.open(img_fpath).convert('RGB')  # for color image\n",
    "        else:\n",
    "            img = Image.open(img_fpath).convert('L')\n",
    "\n",
    "        if not self.opt.sensitive:\n",
    "            label = label.lower()\n",
    "\n",
    "        # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n",
    "        out_of_char = f'[^{self.opt.character}]'\n",
    "        label = re.sub(out_of_char, '', label)\n",
    "\n",
    "        return (img, label)\n",
    "\n",
    "class ResizeNormalize(object):\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = img.resize(self.size, self.interpolation)\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        return img\n",
    "\n",
    "\n",
    "class NormalizePAD(object):\n",
    "\n",
    "    def __init__(self, max_size, PAD_type='right'):\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.max_size = max_size\n",
    "        self.max_width_half = math.floor(max_size[2] / 2)\n",
    "        self.PAD_type = PAD_type\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        c, h, w = img.size()\n",
    "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
    "        Pad_img[:, :, :w] = img  # right pad\n",
    "        if self.max_size[2] != w:  # add border Pad\n",
    "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
    "\n",
    "        return Pad_img\n",
    "\n",
    "\n",
    "class AlignCollate(object):\n",
    "\n",
    "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, contrast_adjust = 0.):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
    "        self.contrast_adjust = contrast_adjust\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = filter(lambda x: x is not None, batch)\n",
    "        images, labels = zip(*batch)\n",
    "\n",
    "        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n",
    "            resized_max_w = self.imgW\n",
    "            input_channel = 3 if images[0].mode == 'RGB' else 1\n",
    "            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
    "\n",
    "            resized_images = []\n",
    "            for image in images:\n",
    "                w, h = image.size\n",
    "\n",
    "                #### augmentation here - change contrast\n",
    "                if self.contrast_adjust > 0:\n",
    "                    image = np.array(image.convert(\"L\"))\n",
    "                    image = adjust_contrast_grey(image, target = self.contrast_adjust)\n",
    "                    image = Image.fromarray(image, 'L')\n",
    "\n",
    "                ratio = w / float(h)\n",
    "                if math.ceil(self.imgH * ratio) > self.imgW:\n",
    "                    resized_w = self.imgW\n",
    "                else:\n",
    "                    resized_w = math.ceil(self.imgH * ratio)\n",
    "\n",
    "                resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
    "                resized_images.append(transform(resized_image))\n",
    "                # resized_image.save('./image_test/%d_test.jpg' % w)\n",
    "\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
    "\n",
    "        else:\n",
    "            transform = ResizeNormalize((self.imgW, self.imgH))\n",
    "            image_tensors = [transform(image) for image in images]\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
    "\n",
    "        return image_tensors, labels\n",
    "\n",
    "\n",
    "def tensor2im(image_tensor, imtype=np.uint8):\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        output = model(image_tensor)  # Use GPU\n",
    "    # ... other GPU ops\n",
    "\n",
    "    # Now it's safe to move to CPU for visualization or saving\n",
    "    image_numpy = image_tensor.detach().cpu().float().numpy()\n",
    "    if image_numpy.shape[0] == 1:\n",
    "        image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "\n",
    "def save_image(image_numpy, image_path):\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b702f859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:11.563228Z",
     "iopub.status.busy": "2025-04-30T08:07:11.563008Z",
     "iopub.status.idle": "2025-04-30T08:07:16.498951Z",
     "shell.execute_reply": "2025-04-30T08:07:16.498172Z"
    },
    "papermill": {
     "duration": 4.945568,
     "end_time": "2025-04-30T08:07:16.500412",
     "exception": false,
     "start_time": "2025-04-30T08:07:11.554844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import six\n",
    "import math\n",
    "import torch\n",
    "import pandas  as pd\n",
    "\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
    "from itertools import accumulate\n",
    "import torchvision.transforms as transforms\n",
    "device = torch.device('cuda')\n",
    "def contrast_grey(img):\n",
    "    high = np.percentile(img, 90)\n",
    "    low  = np.percentile(img, 10)\n",
    "    return (high-low)/(high+low), high, low\n",
    "\n",
    "def adjust_contrast_grey(img, target = 0.4):\n",
    "    contrast, high, low = contrast_grey(img)\n",
    "    if contrast < target:\n",
    "        img = img.astype(int)\n",
    "        ratio = 200./(high-low)\n",
    "        img = (img - low + 25)*ratio\n",
    "        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Batch_Balanced_Dataset(object):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"\n",
    "        Modulate the data ratio in the batch.\n",
    "        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n",
    "        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n",
    "        \"\"\"\n",
    "        log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a')\n",
    "        dashed_line = '-' * 80\n",
    "        print(dashed_line)\n",
    "        log.write(dashed_line + '\\n')\n",
    "        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n",
    "        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n",
    "        assert len(opt.select_data) == len(opt.batch_ratio)\n",
    "\n",
    "        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust = opt.contrast_adjust)\n",
    "        self.data_loader_list = []\n",
    "        self.dataloader_iter_list = []\n",
    "        batch_size_list = []\n",
    "        Total_batch_size = 0\n",
    "        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n",
    "            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n",
    "            print(dashed_line)\n",
    "            log.write(dashed_line + '\\n')\n",
    "            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n",
    "            total_number_dataset = len(_dataset)\n",
    "            log.write(_dataset_log)\n",
    "\n",
    "            \"\"\"\n",
    "            The total number of data can be modified with opt.total_data_usage_ratio.\n",
    "            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n",
    "            See 4.2 section in our paper.\n",
    "            \"\"\"\n",
    "            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n",
    "            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n",
    "            indices = range(total_number_dataset)\n",
    "            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n",
    "                           for offset, length in zip(accumulate(dataset_split), dataset_split)]\n",
    "            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n",
    "            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n",
    "            print(selected_d_log)\n",
    "            log.write(selected_d_log + '\\n')\n",
    "            batch_size_list.append(str(_batch_size))\n",
    "            Total_batch_size += _batch_size\n",
    "\n",
    "            _data_loader = torch.utils.data.DataLoader(\n",
    "                _dataset, batch_size=_batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=int(opt.workers), #prefetch_factor=2,persistent_workers=True,\n",
    "                collate_fn=_AlignCollate, pin_memory=True)\n",
    "            self.data_loader_list.append(_data_loader)\n",
    "            self.dataloader_iter_list.append(iter(_data_loader))\n",
    "\n",
    "        Total_batch_size_log = f'{dashed_line}\\n'\n",
    "        batch_size_sum = '+'.join(batch_size_list)\n",
    "        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n",
    "        Total_batch_size_log += f'{dashed_line}'\n",
    "        opt.batch_size = Total_batch_size\n",
    "\n",
    "        print(Total_batch_size_log)\n",
    "        log.write(Total_batch_size_log + '\\n')\n",
    "        log.close()\n",
    "\n",
    "    def get_batch(self):\n",
    "        balanced_batch_images = []\n",
    "        balanced_batch_texts = []\n",
    "\n",
    "        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n",
    "            try:\n",
    "                image,text = next(iter(data_loader_iter))\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except StopIteration:\n",
    "                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n",
    "                image, text = next(iter(self.dataloader_iter_list[i]))\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n",
    "\n",
    "        return balanced_batch_images, balanced_batch_texts\n",
    "\n",
    "\n",
    "def hierarchical_dataset(root, opt, select_data='/'):\n",
    "    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n",
    "    dataset_list = []\n",
    "    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n",
    "    print(dataset_log)\n",
    "    dataset_log += '\\n'\n",
    "    for dirpath, dirnames, filenames in os.walk(root+'/'):\n",
    "        if not dirnames:\n",
    "            select_flag = False\n",
    "            for selected_d in select_data:\n",
    "                if selected_d in dirpath:\n",
    "                    select_flag = True\n",
    "                    break\n",
    "\n",
    "            if select_flag:\n",
    "                dataset = OCRDataset(dirpath, opt)\n",
    "                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n",
    "                print(sub_dataset_log)\n",
    "                dataset_log += f'{sub_dataset_log}\\n'\n",
    "                dataset_list.append(dataset)\n",
    "\n",
    "    concatenated_dataset = ConcatDataset(dataset_list)\n",
    "\n",
    "    return concatenated_dataset, dataset_log\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, opt):\n",
    "\n",
    "        self.root = root\n",
    "        self.opt = opt\n",
    "        print(root)\n",
    "        self.df = pd.read_csv(os.path.join(root,'labels.csv'), sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "        self.nSamples = len(self.df)\n",
    "\n",
    "        if self.opt.data_filtering_off:\n",
    "            self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n",
    "        else:\n",
    "            self.filtered_index_list = []\n",
    "            for index in range(self.nSamples):\n",
    "                label = self.df.at[index,'words']\n",
    "                try:\n",
    "                    if len(label) > self.opt.batch_max_length:\n",
    "                        continue\n",
    "                except:\n",
    "                    print(label)\n",
    "                out_of_char = f'[^{self.opt.character}]'\n",
    "                if re.search(out_of_char, label.lower()):\n",
    "                    continue\n",
    "                self.filtered_index_list.append(index)\n",
    "            self.nSamples = len(self.filtered_index_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.filtered_index_list[index]\n",
    "        img_fname = self.df.at[index,'filename']\n",
    "        img_fpath = os.path.join(self.root, img_fname)\n",
    "        label = self.df.at[index,'words']\n",
    "\n",
    "        if self.opt.rgb:\n",
    "            img = Image.open(img_fpath).convert('RGB')  # for color image\n",
    "        else:\n",
    "            img = Image.open(img_fpath).convert('L')\n",
    "\n",
    "        if not self.opt.sensitive:\n",
    "            label = label.lower()\n",
    "\n",
    "        # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n",
    "        out_of_char = f'[^{self.opt.character}]'\n",
    "        label = re.sub(out_of_char, '', label)\n",
    "\n",
    "        return (img, label)\n",
    "\n",
    "class ResizeNormalize(object):\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = img.resize(self.size, self.interpolation)\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        return img\n",
    "\n",
    "\n",
    "class NormalizePAD(object):\n",
    "\n",
    "    def __init__(self, max_size, PAD_type='right'):\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.max_size = max_size\n",
    "        self.max_width_half = math.floor(max_size[2] / 2)\n",
    "        self.PAD_type = PAD_type\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        c, h, w = img.size()\n",
    "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
    "        Pad_img[:, :, :w] = img  # right pad\n",
    "        if self.max_size[2] != w:  # add border Pad\n",
    "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
    "\n",
    "        return Pad_img\n",
    "\n",
    "\n",
    "class AlignCollate(object):\n",
    "\n",
    "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, contrast_adjust = 0.):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
    "        self.contrast_adjust = contrast_adjust\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = filter(lambda x: x is not None, batch)\n",
    "        images, labels = zip(*batch)\n",
    "\n",
    "        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n",
    "            resized_max_w = self.imgW\n",
    "            input_channel = 3 if images[0].mode == 'RGB' else 1\n",
    "            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
    "\n",
    "            resized_images = []\n",
    "            for image in images:\n",
    "                w, h = image.size\n",
    "\n",
    "                #### augmentation here - change contrast\n",
    "                if self.contrast_adjust > 0:\n",
    "                    image = np.array(image.convert(\"L\"))\n",
    "                    image = adjust_contrast_grey(image, target = self.contrast_adjust)\n",
    "                    image = Image.fromarray(image, 'L')\n",
    "\n",
    "                ratio = w / float(h)\n",
    "                if math.ceil(self.imgH * ratio) > self.imgW:\n",
    "                    resized_w = self.imgW\n",
    "                else:\n",
    "                    resized_w = math.ceil(self.imgH * ratio)\n",
    "\n",
    "                resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
    "                resized_images.append(transform(resized_image))\n",
    "                # resized_image.save('./image_test/%d_test.jpg' % w)\n",
    "\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
    "\n",
    "        else:\n",
    "            transform = ResizeNormalize((self.imgW, self.imgH))\n",
    "            image_tensors = [transform(image) for image in images]\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
    "\n",
    "        return image_tensors, labels\n",
    "\n",
    "\n",
    "def tensor2im(image_tensor, imtype=np.uint8):\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        output = model(image_tensor)  # Use GPU\n",
    "    # ... other GPU ops\n",
    "\n",
    "    # Now it's safe to move to CPU for visualization or saving\n",
    "    image_numpy = image_tensor.detach().cpu().float().numpy()\n",
    "    if image_numpy.shape[0] == 1:\n",
    "        image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "\n",
    "def save_image(image_numpy, image_path):\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fa13934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:16.517217Z",
     "iopub.status.busy": "2025-04-30T08:07:16.516839Z",
     "iopub.status.idle": "2025-04-30T08:07:16.523221Z",
     "shell.execute_reply": "2025-04-30T08:07:16.522444Z"
    },
    "papermill": {
     "duration": 0.016153,
     "end_time": "2025-04-30T08:07:16.524349",
     "exception": false,
     "start_time": "2025-04-30T08:07:16.508196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/test.py\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "from utils import CTCLabelConverter, AttnLabelConverter, Averager\n",
    "from dataset import hierarchical_dataset, AlignCollate\n",
    "from model import Model\n",
    "\n",
    "\n",
    "def validation(model, criterion, evaluation_loader, converter, opt, device):\n",
    "    \"\"\" validation or evaluation \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    n_correct = 0\n",
    "    norm_ED = 0\n",
    "    length_of_data = 0\n",
    "    infer_time = 0\n",
    "    valid_loss_avg = Averager()\n",
    "\n",
    "    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n",
    "        batch_size = image_tensors.size(0)\n",
    "        length_of_data = length_of_data + batch_size\n",
    "        image = image_tensors.to(device)\n",
    "        # For max length prediction\n",
    "        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "        text_for_loss, length_for_loss = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "        text_for_loss = text_for_loss.to(device)\n",
    "        length_for_loss = length_for_loss.to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        if 'CTC' in opt.Prediction:\n",
    "            preds = model(image, text_for_pred)\n",
    "            forward_time = time.time() - start_time\n",
    "\n",
    "            # Calculate evaluation loss for CTC decoder.\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size).to(device)\n",
    "            # permute 'preds' to use CTCloss format\n",
    "            log_probs = preds.log_softmax(2).permute(1, 0, 2)\n",
    "            cost = criterion(log_probs, text_for_loss, preds_size, length_for_loss)\n",
    "\n",
    "\n",
    "            if opt.decode == 'greedy':\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n",
    "            elif opt.decode == 'beamsearch':\n",
    "                preds_str = converter.decode_beamsearch(preds, beamWidth=2)\n",
    "\n",
    "        else:\n",
    "            preds = model(image, text_for_pred, is_train=False)\n",
    "            forward_time = time.time() - start_time\n",
    "\n",
    "            preds = preds[:, :text_for_loss.shape[1] - 1, :]\n",
    "            target = text_for_loss[:, 1:]  # without [GO] Symbol\n",
    "            cost = criterion(preds.contiguous().view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "\n",
    "            # select max probabilty (greedy decoding) then decode index to character\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_str = converter.decode(preds_index, length_for_pred)\n",
    "            labels = converter.decode(text_for_loss[:, 1:], length_for_loss)\n",
    "\n",
    "        infer_time += forward_time\n",
    "        valid_loss_avg.add(cost)\n",
    "\n",
    "        # calculate accuracy & confidence score\n",
    "        preds_prob = F.softmax(preds, dim=2)\n",
    "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "        confidence_score_list = []\n",
    "        \n",
    "        for gt, pred, pred_max_prob in zip(labels, preds_str, preds_max_prob):\n",
    "            if 'Attn' in opt.Prediction:\n",
    "                gt = gt[:gt.find('[s]')]\n",
    "                pred_EOS = pred.find('[s]')\n",
    "                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "            if pred == gt:\n",
    "                n_correct += 1\n",
    "\n",
    "            '''\n",
    "            (old version) ICDAR2017 DOST Normalized Edit Distance https://rrc.cvc.uab.es/?ch=7&com=tasks\n",
    "            \"For each word we calculate the normalized edit distance to the length of the ground truth transcription.\" \n",
    "            if len(gt) == 0:\n",
    "                norm_ED += 1\n",
    "            else:\n",
    "                norm_ED += edit_distance(pred, gt) / len(gt)\n",
    "            '''\n",
    "            \n",
    "            # ICDAR2019 Normalized Edit Distance \n",
    "            if len(gt) == 0 or len(pred) ==0:\n",
    "                norm_ED += 0\n",
    "            elif len(gt) > len(pred):\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n",
    "            else:\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n",
    "\n",
    "            # calculate confidence score (= multiply of pred_max_prob)\n",
    "            try:\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "            except:\n",
    "                confidence_score = 0  # for empty pred case, when prune after \"end of sentence\" token ([s])\n",
    "            confidence_score_list.append(confidence_score)\n",
    "            # print(pred, gt, pred==gt, confidence_score)\n",
    "\n",
    "    accuracy = n_correct / float(length_of_data) * 100\n",
    "    norm_ED = norm_ED / float(length_of_data) # ICDAR2019 Normalized Edit Distance\n",
    "\n",
    "    return valid_loss_avg.val(), accuracy, norm_ED, preds_str, confidence_score_list, labels, infer_time, length_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a009995e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:16.540258Z",
     "iopub.status.busy": "2025-04-30T08:07:16.540008Z",
     "iopub.status.idle": "2025-04-30T08:07:18.470035Z",
     "shell.execute_reply": "2025-04-30T08:07:18.469278Z"
    },
    "papermill": {
     "duration": 1.939547,
     "end_time": "2025-04-30T08:07:18.471407",
     "exception": false,
     "start_time": "2025-04-30T08:07:16.531860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import string\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "from utils import CTCLabelConverter, AttnLabelConverter, Averager\n",
    "from dataset import hierarchical_dataset, AlignCollate\n",
    "from model import Model\n",
    "\n",
    "def validation(model, criterion, evaluation_loader, converter, opt, device):\n",
    "    \"\"\" validation or evaluation \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    n_correct = 0\n",
    "    norm_ED = 0\n",
    "    length_of_data = 0\n",
    "    infer_time = 0\n",
    "    valid_loss_avg = Averager()\n",
    "\n",
    "    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n",
    "        batch_size = image_tensors.size(0)\n",
    "        length_of_data = length_of_data + batch_size\n",
    "        image = image_tensors.to(device)\n",
    "        # For max length prediction\n",
    "        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "        text_for_loss, length_for_loss = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "        text_for_loss = text_for_loss.to(device)\n",
    "        length_for_loss = length_for_loss.to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        if 'CTC' in opt.Prediction:\n",
    "            preds = model(image, text_for_pred)\n",
    "            forward_time = time.time() - start_time\n",
    "\n",
    "            # Calculate evaluation loss for CTC decoder.\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size).to(device)\n",
    "            # permute 'preds' to use CTCloss format\n",
    "            log_probs = preds.log_softmax(2).permute(1, 0, 2)\n",
    "            cost = criterion(log_probs, text_for_loss, preds_size, length_for_loss)\n",
    "\n",
    "            if opt.decode == 'greedy':\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n",
    "            elif opt.decode == 'beamsearch':\n",
    "                preds_str = converter.decode_beamsearch(preds, beamWidth=2)\n",
    "\n",
    "        else:\n",
    "            preds = model(image, text_for_pred, is_train=False)\n",
    "            forward_time = time.time() - start_time\n",
    "\n",
    "            preds = preds[:, :text_for_loss.shape[1] - 1, :]\n",
    "            target = text_for_loss[:, 1:]  # without [GO] Symbol\n",
    "            cost = criterion(preds.contiguous().view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "\n",
    "            # select max probabilty (greedy decoding) then decode index to character\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_str = converter.decode(preds_index, length_for_pred)\n",
    "            labels = converter.decode(text_for_loss[:, 1:], length_for_loss)\n",
    "\n",
    "        infer_time += forward_time\n",
    "        valid_loss_avg.add(cost)\n",
    "\n",
    "        # calculate accuracy & confidence score\n",
    "        preds_prob = F.softmax(preds, dim=2)\n",
    "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "        confidence_score_list = []\n",
    "        \n",
    "        for gt, pred, pred_max_prob in zip(labels, preds_str, preds_max_prob):\n",
    "            if 'Attn' in opt.Prediction:\n",
    "                gt = gt[:gt.find('[s]')]\n",
    "                pred_EOS = pred.find('[s]')\n",
    "                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "            if pred == gt:\n",
    "                n_correct += 1\n",
    "\n",
    "            '''\n",
    "            (old version) ICDAR2017 DOST Normalized Edit Distance https://rrc.cvc.uab.es/?ch=7&com=tasks\n",
    "            \"For each word we calculate the normalized edit distance to the length of the ground truth transcription.\" \n",
    "            if len(gt) == 0:\n",
    "                norm_ED += 1\n",
    "            else:\n",
    "                norm_ED += edit_distance(pred, gt) / len(gt)\n",
    "            '''\n",
    "            \n",
    "            # ICDAR2019 Normalized Edit Distance \n",
    "            if len(gt) == 0 or len(pred) ==0:\n",
    "                norm_ED += 0\n",
    "            elif len(gt) > len(pred):\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n",
    "            else:\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n",
    "\n",
    "            # calculate confidence score (= multiply of pred_max_prob)\n",
    "            try:\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "            except:\n",
    "                confidence_score = 0  # for empty pred case, when prune after \"end of sentence\" token ([s])\n",
    "            confidence_score_list.append(confidence_score)\n",
    "            # print(pred, gt, pred==gt, confidence_score)\n",
    "\n",
    "    accuracy = n_correct / float(length_of_data) * 100\n",
    "    norm_ED = norm_ED / float(length_of_data) # ICDAR2019 Normalized Edit Distance\n",
    "\n",
    "    return valid_loss_avg.val(), accuracy, norm_ED, preds_str, confidence_score_list, labels, infer_time, length_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51f2539d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:18.491498Z",
     "iopub.status.busy": "2025-04-30T08:07:18.491142Z",
     "iopub.status.idle": "2025-04-30T08:07:18.500254Z",
     "shell.execute_reply": "2025-04-30T08:07:18.499433Z"
    },
    "papermill": {
     "duration": 0.01938,
     "end_time": "2025-04-30T08:07:18.501343",
     "exception": false,
     "start_time": "2025-04-30T08:07:18.481963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/train.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import string\n",
    "import argparse\n",
    "\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from utils import CTCLabelConverter, AttnLabelConverter, Averager\n",
    "from dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\n",
    "from model import Model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def count_parameters(model):\n",
    "    print(\"Modules, Parameters\")\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        #table.add_row([name, param])\n",
    "        total_params+=param\n",
    "        print(name, param)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "def train(opt, show_number = 2, amp=False):\n",
    "    \"\"\" dataset preparation \"\"\"\n",
    "    if not opt.data_filtering_off:\n",
    "        print('Filtering the images containing characters which are not in opt.character')\n",
    "        print('Filtering the images whose label is longer than opt.batch_max_length')\n",
    "\n",
    "    opt.select_data = opt.select_data.split('-')\n",
    "    opt.batch_ratio = opt.batch_ratio.split('-')\n",
    "    train_dataset = Batch_Balanced_Dataset(opt)\n",
    "    \n",
    "    log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a', encoding=\"utf8\")\n",
    "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust=opt.contrast_adjust)\n",
    "    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=min(32, opt.batch_size),\n",
    "        shuffle=True,  # 'True' to check training progress with validation function.\n",
    "        num_workers=int(opt.workers), prefetch_factor=512,\n",
    "        collate_fn=AlignCollate_valid, pin_memory=True)\n",
    "    log.write(valid_dataset_log)\n",
    "    print('-' * 80)\n",
    "    log.write('-' * 80 + '\\n')\n",
    "    log.close()\n",
    "    \n",
    "    \"\"\" model configuration \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "    model = Model(opt)\n",
    "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "          opt.SequenceModeling, opt.Prediction)\n",
    "\n",
    "    if opt.saved_model != '':\n",
    "        pretrained_dict = torch.load(opt.saved_model)\n",
    "        if opt.new_prediction:\n",
    "            model.Prediction = nn.Linear(model.SequenceModeling_output, len(pretrained_dict['module.Prediction.weight']))  \n",
    "        \n",
    "        model = torch.nn.DataParallel(model).to(device) \n",
    "        print(f'loading pretrained model from {opt.saved_model}')\n",
    "        if opt.FT:\n",
    "            model.load_state_dict(pretrained_dict, strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(pretrained_dict)\n",
    "        if opt.new_prediction:\n",
    "            model.module.Prediction = nn.Linear(model.module.SequenceModeling_output, opt.num_class)  \n",
    "            for name, param in model.module.Prediction.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            model = model.to(device) \n",
    "    else:\n",
    "        # weight initialization\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'localization_fc2' in name:\n",
    "                print(f'Skip {name} as it is already initialized')\n",
    "                continue\n",
    "            try:\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            except Exception as e:  # for batchnorm.\n",
    "                if 'weight' in name:\n",
    "                    param.data.fill_(1)\n",
    "                continue\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "    \n",
    "    model.train() \n",
    "    print(\"Model:\")\n",
    "    print(model)\n",
    "    count_parameters(model)\n",
    "    \n",
    "    \"\"\" setup loss \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n",
    "    # loss averager\n",
    "    loss_avg = Averager()\n",
    "\n",
    "    # freeze some layers\n",
    "    try:\n",
    "        if opt.freeze_FeatureFxtraction:\n",
    "            for param in model.module.FeatureExtraction.parameters():\n",
    "                param.requires_grad = False\n",
    "        if opt.freeze_SequenceModeling:\n",
    "            for param in model.module.SequenceModeling.parameters():\n",
    "                param.requires_grad = False\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # filter that only require gradient decent\n",
    "    filtered_parameters = []\n",
    "    params_num = []\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        filtered_parameters.append(p)\n",
    "        params_num.append(np.prod(p.size()))\n",
    "    print('Trainable params num : ', sum(params_num))\n",
    "    # [print(name, p.numel()) for name, p in filter(lambda p: p[1].requires_grad, model.named_parameters())]\n",
    "\n",
    "    # setup optimizer\n",
    "    if opt.optim=='adam':\n",
    "        #optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "        optimizer = optim.Adam(filtered_parameters)\n",
    "    else:\n",
    "        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n",
    "    print(\"Optimizer:\")\n",
    "    print(optimizer)\n",
    "\n",
    "    \"\"\" final options \"\"\"\n",
    "    # print(opt)\n",
    "    with open(f'./saved_models/{opt.experiment_name}/opt.txt', 'a', encoding=\"utf8\") as opt_file:\n",
    "        opt_log = '------------ Options -------------\\n'\n",
    "        args = vars(opt)\n",
    "        for k, v in args.items():\n",
    "            opt_log += f'{str(k)}: {str(v)}\\n'\n",
    "        opt_log += '---------------------------------------\\n'\n",
    "        print(opt_log)\n",
    "        opt_file.write(opt_log)\n",
    "\n",
    "    \"\"\" start training \"\"\"\n",
    "    start_iter = 0\n",
    "    if opt.saved_model != '':\n",
    "        try:\n",
    "            start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n",
    "            print(f'continue to train, start_iter: {start_iter}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_accuracy = -1\n",
    "    best_norm_ED = -1\n",
    "    i = start_iter\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    t1= time.time()\n",
    "        \n",
    "    while(True):\n",
    "        # train part\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if amp:\n",
    "            with autocast():\n",
    "                image_tensors, labels = train_dataset.get_batch()\n",
    "                image = image_tensors.to(device)\n",
    "                text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "                batch_size = image.size(0)\n",
    "\n",
    "                if 'CTC' in opt.Prediction:\n",
    "                    preds = model(image, text).log_softmax(2)\n",
    "                    preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                    preds = preds.permute(1, 0, 2)\n",
    "                    torch.backends.cudnn.enabled = False\n",
    "                    cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
    "                    torch.backends.cudnn.enabled = True\n",
    "                else:\n",
    "                    preds = model(image, text[:, :-1])  # align with Attention.forward\n",
    "                    target = text[:, 1:]  # without [GO] Symbol\n",
    "                    cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "            scaler.scale(cost).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            image_tensors, labels = train_dataset.get_batch()\n",
    "            image = image_tensors.to(device)\n",
    "            text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "            batch_size = image.size(0)\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image, text).log_softmax(2)\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                preds = preds.permute(1, 0, 2)\n",
    "                torch.backends.cudnn.enabled = False\n",
    "                cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
    "                torch.backends.cudnn.enabled = True\n",
    "            else:\n",
    "                preds = model(image, text[:, :-1])  # align with Attention.forward\n",
    "                target = text[:, 1:]  # without [GO] Symbol\n",
    "                cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "            cost.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip) \n",
    "            optimizer.step()\n",
    "        loss_avg.add(cost)\n",
    "\n",
    "        # validation part\n",
    "        if (i % opt.valInterval == 0) and (i!=0):\n",
    "            print('training time: ', time.time()-t1)\n",
    "            t1=time.time()\n",
    "            elapsed_time = time.time() - start_time\n",
    "            # for log\n",
    "            with open(f'./saved_models/{opt.experiment_name}/log_train.txt', 'a', encoding=\"utf8\") as log:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels,\\\n",
    "                    infer_time, length_of_data = validation(model, criterion, valid_loader, converter, opt, device)\n",
    "                model.train()\n",
    "\n",
    "                # training loss and validation loss\n",
    "                loss_log = f'[{i}/{opt.num_iter}] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n",
    "                loss_avg.reset()\n",
    "\n",
    "                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.4f}'\n",
    "\n",
    "                # keep best accuracy model (on valid dataset)\n",
    "                if current_accuracy > best_accuracy:\n",
    "                    best_accuracy = current_accuracy\n",
    "                    torch.save(model.state_dict(), f'/kaggle/working/best_accuracy.pth')\n",
    "                if current_norm_ED > best_norm_ED:\n",
    "                    best_norm_ED = current_norm_ED\n",
    "                    torch.save(model.state_dict(), f'/kaggle/working/best_norm_ED.pth')\n",
    "                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.4f}'\n",
    "\n",
    "                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
    "                print(loss_model_log)\n",
    "                log.write(loss_model_log + '\\n')\n",
    "\n",
    "                # show some predicted results\n",
    "                dashed_line = '-' * 80\n",
    "                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n",
    "                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
    "                \n",
    "                #show_number = min(show_number, len(labels))\n",
    "                \n",
    "                start = random.randint(0,len(labels) - show_number )    \n",
    "                for gt, pred, confidence in zip(labels[start:start+show_number], preds[start:start+show_number], confidence_score[start:start+show_number]):\n",
    "                    if 'Attn' in opt.Prediction:\n",
    "                        gt = gt[:gt.find('[s]')]\n",
    "                        pred = pred[:pred.find('[s]')]\n",
    "\n",
    "                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n",
    "                predicted_result_log += f'{dashed_line}'\n",
    "                print(predicted_result_log)\n",
    "                log.write(predicted_result_log + '\\n')\n",
    "                print('validation time: ', time.time()-t1)\n",
    "                t1=time.time()\n",
    "        # save model per 1e+4 iter.\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(), f'./saved_models/{opt.experiment_name}/iter_{i+1}.pth')\n",
    "        if i == opt.num_iter:\n",
    "            print('end the training')\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8f78d05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:18.518287Z",
     "iopub.status.busy": "2025-04-30T08:07:18.518030Z",
     "iopub.status.idle": "2025-04-30T08:07:18.546884Z",
     "shell.execute_reply": "2025-04-30T08:07:18.546087Z"
    },
    "papermill": {
     "duration": 0.038767,
     "end_time": "2025-04-30T08:07:18.548124",
     "exception": false,
     "start_time": "2025-04-30T08:07:18.509357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import string\n",
    "import argparse\n",
    "\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from utils import CTCLabelConverter, AttnLabelConverter, Averager\n",
    "from dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\n",
    "from model import Model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def count_parameters(model):\n",
    "    print(\"Modules, Parameters\")\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        #table.add_row([name, param])\n",
    "        total_params+=param\n",
    "        print(name, param)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "def train(opt, show_number = 2, amp=False):\n",
    "    \"\"\" dataset preparation \"\"\"\n",
    "    if not opt.data_filtering_off:\n",
    "        print('Filtering the images containing characters which are not in opt.character')\n",
    "        print('Filtering the images whose label is longer than opt.batch_max_length')\n",
    "\n",
    "    opt.select_data = opt.select_data.split('-')\n",
    "    opt.batch_ratio = opt.batch_ratio.split('-')\n",
    "    train_dataset = Batch_Balanced_Dataset(opt)\n",
    "    \n",
    "    log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a', encoding=\"utf8\")\n",
    "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust=opt.contrast_adjust)\n",
    "    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=min(32, opt.batch_size),\n",
    "        shuffle=True,  # 'True' to check training progress with validation function.\n",
    "        num_workers=int(opt.workers), prefetch_factor=512,\n",
    "        collate_fn=AlignCollate_valid, pin_memory=True)\n",
    "    log.write(valid_dataset_log)\n",
    "    print('-' * 80)\n",
    "    log.write('-' * 80 + '\\n')\n",
    "    log.close()\n",
    "    \n",
    "    \"\"\" model configuration \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "    model = Model(opt)\n",
    "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "          opt.SequenceModeling, opt.Prediction)\n",
    "\n",
    "    if opt.saved_model != '':\n",
    "        pretrained_dict = torch.load(opt.saved_model)\n",
    "        if opt.new_prediction:\n",
    "            model.Prediction = nn.Linear(model.SequenceModeling_output, len(pretrained_dict['module.Prediction.weight']))  \n",
    "        \n",
    "        model = torch.nn.DataParallel(model).to(device) \n",
    "        print(f'loading pretrained model from {opt.saved_model}')\n",
    "        if opt.FT:\n",
    "            model.load_state_dict(pretrained_dict, strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(pretrained_dict)\n",
    "        if opt.new_prediction:\n",
    "            model.module.Prediction = nn.Linear(model.module.SequenceModeling_output, opt.num_class)  \n",
    "            for name, param in model.module.Prediction.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            model = model.to(device) \n",
    "    else:\n",
    "        # weight initialization\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'localization_fc2' in name:\n",
    "                print(f'Skip {name} as it is already initialized')\n",
    "                continue\n",
    "            try:\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            except Exception as e:  # for batchnorm.\n",
    "                if 'weight' in name:\n",
    "                    param.data.fill_(1)\n",
    "                continue\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "    \n",
    "    model.train() \n",
    "    print(\"Model:\")\n",
    "    print(model)\n",
    "    count_parameters(model)\n",
    "    \n",
    "    \"\"\" setup loss \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n",
    "    # loss averager\n",
    "    loss_avg = Averager()\n",
    "\n",
    "    # freeze some layers\n",
    "    try:\n",
    "        if opt.freeze_FeatureFxtraction:\n",
    "            for param in model.module.FeatureExtraction.parameters():\n",
    "                param.requires_grad = False\n",
    "        if opt.freeze_SequenceModeling:\n",
    "            for param in model.module.SequenceModeling.parameters():\n",
    "                param.requires_grad = False\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # filter that only require gradient decent\n",
    "    filtered_parameters = []\n",
    "    params_num = []\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        filtered_parameters.append(p)\n",
    "        params_num.append(np.prod(p.size()))\n",
    "    print('Trainable params num : ', sum(params_num))\n",
    "    # [print(name, p.numel()) for name, p in filter(lambda p: p[1].requires_grad, model.named_parameters())]\n",
    "\n",
    "    # setup optimizer\n",
    "    if opt.optim=='adam':\n",
    "        #optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "        optimizer = optim.Adam(filtered_parameters)\n",
    "    else:\n",
    "        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n",
    "    print(\"Optimizer:\")\n",
    "    print(optimizer)\n",
    "\n",
    "    \"\"\" final options \"\"\"\n",
    "    # print(opt)\n",
    "    with open(f'./saved_models/{opt.experiment_name}/opt.txt', 'a', encoding=\"utf8\") as opt_file:\n",
    "        opt_log = '------------ Options -------------\\n'\n",
    "        args = vars(opt)\n",
    "        for k, v in args.items():\n",
    "            opt_log += f'{str(k)}: {str(v)}\\n'\n",
    "        opt_log += '---------------------------------------\\n'\n",
    "        print(opt_log)\n",
    "        opt_file.write(opt_log)\n",
    "\n",
    "    \"\"\" start training \"\"\"\n",
    "    start_iter = 0\n",
    "    if opt.saved_model != '':\n",
    "        try:\n",
    "            start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n",
    "            print(f'continue to train, start_iter: {start_iter}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_accuracy = -1\n",
    "    best_norm_ED = -1\n",
    "    i = start_iter\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    t1= time.time()\n",
    "        \n",
    "    while(True):\n",
    "        # train part\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if amp:\n",
    "            with autocast():\n",
    "                image_tensors, labels = train_dataset.get_batch()\n",
    "                image = image_tensors.to(device)\n",
    "                text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "                batch_size = image.size(0)\n",
    "\n",
    "                if 'CTC' in opt.Prediction:\n",
    "                    preds = model(image, text).log_softmax(2)\n",
    "                    preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                    preds = preds.permute(1, 0, 2)\n",
    "                    torch.backends.cudnn.enabled = False\n",
    "                    cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
    "                    torch.backends.cudnn.enabled = True\n",
    "                else:\n",
    "                    preds = model(image, text[:, :-1])  # align with Attention.forward\n",
    "                    target = text[:, 1:]  # without [GO] Symbol\n",
    "                    cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "            scaler.scale(cost).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            image_tensors, labels = train_dataset.get_batch()\n",
    "            image = image_tensors.to(device)\n",
    "            text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "            batch_size = image.size(0)\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image, text).log_softmax(2)\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                preds = preds.permute(1, 0, 2)\n",
    "                torch.backends.cudnn.enabled = False\n",
    "                cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
    "                torch.backends.cudnn.enabled = True\n",
    "            else:\n",
    "                preds = model(image, text[:, :-1])  # align with Attention.forward\n",
    "                target = text[:, 1:]  # without [GO] Symbol\n",
    "                cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "            cost.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip) \n",
    "            optimizer.step()\n",
    "        loss_avg.add(cost)\n",
    "\n",
    "        # validation part\n",
    "        if (i % opt.valInterval == 0) and (i!=0):\n",
    "            model = model.to(device)\n",
    "            print('training time: ', time.time()-t1)\n",
    "            t1=time.time()\n",
    "            elapsed_time = time.time() - start_time\n",
    "            # for log\n",
    "            with open(f'./saved_models/{opt.experiment_name}/log_train.txt', 'a', encoding=\"utf8\") as log:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels,\\\n",
    "                    infer_time, length_of_data = validation(model, criterion, valid_loader, converter, opt, device)\n",
    "                model.train()\n",
    "\n",
    "                # training loss and validation loss\n",
    "                loss_log = f'[{i}/{opt.num_iter}] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n",
    "                loss_avg.reset()\n",
    "\n",
    "                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.4f}'\n",
    "\n",
    "                # keep best accuracy model (on valid dataset)\n",
    "                if current_accuracy > best_accuracy:\n",
    "                    best_accuracy = current_accuracy\n",
    "                    torch.save(model.state_dict(), f'/kaggle/working/best_accuracy.pth')\n",
    "                if current_norm_ED > best_norm_ED:\n",
    "                    best_norm_ED = current_norm_ED\n",
    "                    torch.save(model.state_dict(), f'/kaggle/working/best_norm_ED.pth')\n",
    "                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.4f}'\n",
    "\n",
    "                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
    "                print(loss_model_log)\n",
    "                log.write(loss_model_log + '\\n')\n",
    "\n",
    "                # show some predicted results\n",
    "                dashed_line = '-' * 80\n",
    "                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n",
    "                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
    "                \n",
    "                #show_number = min(show_number, len(labels))\n",
    "                \n",
    "                start = random.randint(0,len(labels) - show_number )    \n",
    "                for gt, pred, confidence in zip(labels[start:start+show_number], preds[start:start+show_number], confidence_score[start:start+show_number]):\n",
    "                    if 'Attn' in opt.Prediction:\n",
    "                        gt = gt[:gt.find('[s]')]\n",
    "                        pred = pred[:pred.find('[s]')]\n",
    "\n",
    "                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n",
    "                predicted_result_log += f'{dashed_line}'\n",
    "                print(predicted_result_log)\n",
    "                log.write(predicted_result_log + '\\n')\n",
    "                print('validation time: ', time.time()-t1)\n",
    "                t1=time.time()\n",
    "        # save model per 1e+4 iter.\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(), f'./saved_models/{opt.experiment_name}/iter_{i+1}.pth')\n",
    "        if i == opt.num_iter:\n",
    "            print('end the training')\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29b464aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T08:07:18.564543Z",
     "iopub.status.busy": "2025-04-30T08:07:18.563875Z",
     "iopub.status.idle": "2025-04-30T08:16:22.034769Z",
     "shell.execute_reply": "2025-04-30T08:16:22.033606Z"
    },
    "papermill": {
     "duration": 543.480412,
     "end_time": "2025-04-30T08:16:22.036123",
     "exception": false,
     "start_time": "2025-04-30T08:07:18.555711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the images containing characters which are not in opt.character\n",
      "Filtering the images whose label is longer than opt.batch_max_length\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root: all_data\n",
      "opt.select_data: ['en_train_filtered']\n",
      "opt.batch_ratio: ['1']\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data\t dataset: en_train_filtered\n",
      "all_data/en_train_filtered/__results___files\n",
      "sub-directory:\t/en_train_filtered/__results___files\t num samples: 876\n",
      "num total samples of en_train_filtered: 876 x 1.0 (total_data_usage_ratio) = 876\n",
      "num samples of en_train_filtered per batch: 16 x 1.0 (batch_ratio) = 16\n",
      "--------------------------------------------------------------------------------\n",
      "Total_batch_size: 16 = 16\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data/en_val\t dataset: /\n",
      "all_data/en_val/__results___files\n",
      "sub-directory:\t/__results___files\t num samples: 46\n",
      "--------------------------------------------------------------------------------\n",
      "No Transformation module specified\n",
      "model input parameters 64 600 20 1 256 256 97 34 None VGG BiLSTM CTC\n",
      "Model:\n",
      "DataParallel(\n",
      "  (module): Model(\n",
      "    (FeatureExtraction): VGG_FeatureExtractor(\n",
      "      (ConvNet): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): ReLU(inplace=True)\n",
      "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (7): ReLU(inplace=True)\n",
      "        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (9): ReLU(inplace=True)\n",
      "        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "        (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (13): ReLU(inplace=True)\n",
      "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (16): ReLU(inplace=True)\n",
      "        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "        (18): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "        (19): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
      "    (SequenceModeling): Sequential(\n",
      "      (0): BidirectionalLSTM(\n",
      "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (1): BidirectionalLSTM(\n",
      "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (Prediction): Linear(in_features=256, out_features=97, bias=True)\n",
      "  )\n",
      ")\n",
      "Modules, Parameters\n",
      "module.FeatureExtraction.ConvNet.0.weight 288\n",
      "module.FeatureExtraction.ConvNet.0.bias 32\n",
      "module.FeatureExtraction.ConvNet.3.weight 18432\n",
      "module.FeatureExtraction.ConvNet.3.bias 64\n",
      "module.FeatureExtraction.ConvNet.6.weight 73728\n",
      "module.FeatureExtraction.ConvNet.6.bias 128\n",
      "module.FeatureExtraction.ConvNet.8.weight 147456\n",
      "module.FeatureExtraction.ConvNet.8.bias 128\n",
      "module.FeatureExtraction.ConvNet.11.weight 294912\n",
      "module.FeatureExtraction.ConvNet.12.weight 256\n",
      "module.FeatureExtraction.ConvNet.12.bias 256\n",
      "module.FeatureExtraction.ConvNet.14.weight 589824\n",
      "module.FeatureExtraction.ConvNet.15.weight 256\n",
      "module.FeatureExtraction.ConvNet.15.bias 256\n",
      "module.FeatureExtraction.ConvNet.18.weight 262144\n",
      "module.FeatureExtraction.ConvNet.18.bias 256\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.0.linear.weight 131072\n",
      "module.SequenceModeling.0.linear.bias 256\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.1.linear.weight 131072\n",
      "module.SequenceModeling.1.linear.bias 256\n",
      "module.Prediction.weight 24832\n",
      "module.Prediction.bias 97\n",
      "Total Trainable Params: 3781345\n",
      "Trainable params num :  3781345\n",
      "Optimizer:\n",
      "Adadelta (\n",
      "Parameter Group 0\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 1.0\n",
      "    maximize: False\n",
      "    rho: 0.95\n",
      "    weight_decay: 0\n",
      ")\n",
      "------------ Options -------------\n",
      "number: 0123456789\n",
      "symbol: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €\n",
      "lang_char: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "experiment_name: en_filtered\n",
      "train_data: all_data\n",
      "valid_data: all_data/en_val\n",
      "manualSeed: 1111\n",
      "workers: 4\n",
      "batch_size: 16\n",
      "num_iter: 10000\n",
      "valInterval: 1000\n",
      "saved_model: \n",
      "FT: False\n",
      "optim: False\n",
      "lr: 1.0\n",
      "beta1: 0.9\n",
      "rho: 0.95\n",
      "eps: 1e-08\n",
      "grad_clip: 5\n",
      "select_data: ['en_train_filtered']\n",
      "batch_ratio: ['1']\n",
      "total_data_usage_ratio: 1.0\n",
      "batch_max_length: 34\n",
      "imgH: 64\n",
      "imgW: 600\n",
      "rgb: False\n",
      "contrast_adjust: 0.0\n",
      "sensitive: True\n",
      "PAD: True\n",
      "data_filtering_off: False\n",
      "Transformation: None\n",
      "FeatureExtraction: VGG\n",
      "SequenceModeling: BiLSTM\n",
      "Prediction: CTC\n",
      "num_fiducial: 20\n",
      "input_channel: 1\n",
      "output_channel: 256\n",
      "hidden_size: 256\n",
      "decode: greedy\n",
      "new_prediction: False\n",
      "freeze_FeatureFxtraction: False\n",
      "freeze_SequenceModeling: False\n",
      "character: 0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "num_class: 97\n",
      "---------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/4257424241.py:176: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time:  57.9164776802063\n",
      "[1000/10000] Train loss: 3.61268, Valid loss: 1.81738, Elapsed_time: 57.91712\n",
      "Current_accuracy : 6.522, Current_norm_ED  : 0.5256\n",
      "Best_accuracy    : 6.522, Best_norm_ED     : 0.5256\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "whishes Jews Lunenburg $  | Piiees es tn re           | 0.0000\tFalse\n",
      "nikah Jutland_blissfully  | Hrd K                     | 0.0000\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.8746297359466553\n",
      "training time:  53.26343488693237\n",
      "[2000/10000] Train loss: 0.52231, Valid loss: 0.44033, Elapsed_time: 112.05557\n",
      "Current_accuracy : 39.130, Current_norm_ED  : 0.8878\n",
      "Best_accuracy    : 39.130, Best_norm_ED     : 0.8878\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "Lindsey_Coge Vaqueros nuns | {indley Coqe [Bqqual Bnuns | 0.0000\tFalse\n",
      "Aic VIPAKAYA              | Aic VIPAKAYxA             | 0.0228\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.5653176307678223\n",
      "training time:  53.18537425994873\n",
      "[3000/10000] Train loss: 0.09205, Valid loss: 0.00447, Elapsed_time: 165.80655\n",
      "Current_accuracy : 100.000, Current_norm_ED  : 1.0000\n",
      "Best_accuracy    : 100.000, Best_norm_ED     : 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "Dayari                    | Dayari                    | 0.6775\tTrue\n",
      "~Bombed Discourse         | ~Bombed Discourse         | 0.2105\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.5217454433441162\n",
      "training time:  53.226428270339966\n",
      "[4000/10000] Train loss: 0.02747, Valid loss: 0.00104, Elapsed_time: 219.55499\n",
      "Current_accuracy : 100.000, Current_norm_ED  : 1.0000\n",
      "Best_accuracy    : 100.000, Best_norm_ED     : 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "Innocent Westlake Aeschinus | Innocent Westlake Aeschinus | 0.1507\tTrue\n",
      "Corvallis Jakwob          | Corvallis Jakwob          | 0.2284\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.4744760990142822\n",
      "training time:  53.44614315032959\n",
      "[5000/10000] Train loss: 0.00135, Valid loss: 0.00025, Elapsed_time: 273.47589\n",
      "Current_accuracy : 100.000, Current_norm_ED  : 1.0000\n",
      "Best_accuracy    : 100.000, Best_norm_ED     : 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "Swamy Carys Personified   | Swamy Carys Personified   | 0.0682\tTrue\n",
      "Highs {Orecta}            | Highs {Orecta}            | 0.4529\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.46561121940612793\n",
      "training time:  53.382959604263306\n",
      "[6000/10000] Train loss: 0.00073, Valid loss: 0.00011, Elapsed_time: 327.32475\n",
      "Current_accuracy : 100.000, Current_norm_ED  : 1.0000\n",
      "Best_accuracy    : 100.000, Best_norm_ED     : 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "Dayari                    | Dayari                    | 0.9427\tTrue\n",
      "Baylor ?                  | Baylor ?                  | 0.5915\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.46854352951049805\n",
      "training time:  53.290809869766235\n",
      "[7000/10000] Train loss: 0.00054, Valid loss: 0.00008, Elapsed_time: 381.08441\n",
      "Current_accuracy : 100.000, Current_norm_ED  : 1.0000\n",
      "Best_accuracy    : 100.000, Best_norm_ED     : 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "CONSTELLATION             | CONSTELLATION             | 0.9025\tTrue\n",
      "Onkilon RMS |             | Onkilon RMS |             | 0.1644\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.4857194423675537\n",
      "training time:  53.21829438209534\n",
      "[8000/10000] Train loss: 0.00049, Valid loss: 0.00006, Elapsed_time: 434.78872\n",
      "Current_accuracy : 100.000, Current_norm_ED  : 1.0000\n",
      "Best_accuracy    : 100.000, Best_norm_ED     : 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "Gojo-Barocci? . Walt      | Gojo-Barocci? . Walt      | 0.3769\tTrue\n",
      "Innocent Westlake Aeschinus | Innocent Westlake Aeschinus | 0.1219\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.4993305206298828\n",
      "training time:  53.332093715667725\n",
      "[9000/10000] Train loss: 0.00044, Valid loss: 0.00005, Elapsed_time: 488.62045\n",
      "Current_accuracy : 100.000, Current_norm_ED  : 1.0000\n",
      "Best_accuracy    : 100.000, Best_norm_ED     : 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "BALTIC :                  | BALTIC :                  | 0.5072\tTrue\n",
      "Andrea Gazoo              | Andrea Gazoo              | 0.7209\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.47783327102661133\n",
      "training time:  53.3215115070343\n",
      "[10000/10000] Train loss: 0.00043, Valid loss: 0.00005, Elapsed_time: 542.42005\n",
      "Current_accuracy : 100.000, Current_norm_ED  : 1.0000\n",
      "Best_accuracy    : 100.000, Best_norm_ED     : 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "Gojo-Barocci? . Walt      | Gojo-Barocci? . Walt      | 0.4757\tTrue\n",
      "Jinglei @ Extensive       | Jinglei @ Extensive       | 0.2599\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.4644031524658203\n",
      "end the training\n"
     ]
    }
   ],
   "source": [
    "train(opt, amp=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba35d64",
   "metadata": {
    "papermill": {
     "duration": 0.008188,
     "end_time": "2025-04-30T08:16:22.053533",
     "exception": false,
     "start_time": "2025-04-30T08:16:22.045345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7125783,
     "sourceId": 11424699,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7195599,
     "sourceId": 11480663,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 589.595331,
   "end_time": "2025-04-30T08:16:25.668978",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-30T08:06:36.073647",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
